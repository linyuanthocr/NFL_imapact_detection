{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"papermill":{"duration":5672.030464,"end_time":"2020-12-03T21:15:43.903691","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2020-12-03T19:41:11.873227","version":"2.1.0"},"colab":{"name":"2class-object-detection-training-gkfold-cutmix.ipynb","provenance":[{"file_id":"1hekQQEdhAtf042ATqI5Y859gudARY0Ct","timestamp":1607286130579}],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"papermill":{"duration":0.016133,"end_time":"2020-12-03T19:41:16.277625","exception":false,"start_time":"2020-12-03T19:41:16.261492","status":"completed"},"tags":[],"id":"SbAt3SlE5Qu8"},"source":["This notebook detects 2 class objects.\n","- class1: helmet without impact\n","- class2: helmet with impact\n","\n","Object Detection part is based on [EfficientDet notebook](https://www.kaggle.com/shonenkov/training-efficientdet) for [global wheat detection competition](https://www.kaggle.com/c/global-wheat-detection) by [shonenkov](https://www.kaggle.com/shonenkov), which is using [github repos efficientdet-pytorch](https://github.com/rwightman/efficientdet-pytorch) by [@rwightman](https://www.kaggle.com/rwightman).\n","\n","Inference part can be foud [here](https://www.kaggle.com/its7171/2class-object-detection-inference/)."]},{"cell_type":"code","metadata":{"id":"FkvTg6_c6Cj0"},"source":["Kaggle = False\n","Colab = !Kaggle"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JmIZ8aCDBp6-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608506418153,"user_tz":480,"elapsed":21148,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}},"outputId":"5e0610f7-c897-4e64-c388-2b7a76845166"},"source":["import os, sys\n","from pathlib import Path\n","\n","if Colab:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","\n","    path = \"/content/drive/My Drive\"\n","    os.chdir(path)\n","    os.listdir(path)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2020-12-03T19:41:16.315908Z","iopub.status.busy":"2020-12-03T19:41:16.314887Z","iopub.status.idle":"2020-12-03T19:41:47.853592Z","shell.execute_reply":"2020-12-03T19:41:47.852000Z"},"papermill":{"duration":31.561437,"end_time":"2020-12-03T19:41:47.853762","exception":false,"start_time":"2020-12-03T19:41:16.292325","status":"completed"},"tags":[],"id":"b33SLkXj5Qu8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608506439422,"user_tz":480,"elapsed":42406,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}},"outputId":"49ac67af-692c-4249-d377-997d3555ac7d"},"source":["if Kaggle:\n","    !pip install ../input/nfl-lib/timm-0.1.26-py3-none-any.whl\n","    !tar xfz ../input/nfl-lib/pkgs.tgz\n","else:\n","    !pip install NFL/nfl-lib/timm-0.1.26-py3-none-any.whl\n","    !tar xfz NFL/nfl-lib/pkgs.tgz    \n","    !pip install albumentations==0.4.6\n","# for pytorch1.6\n","cmd = \"sed -i -e 's/ \\/ / \\/\\/ /' timm-efficientdet-pytorch/effdet/bench.py\"\n","!$cmd"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Processing ./NFL/nfl-lib/timm-0.1.26-py3-none-any.whl\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from timm==0.1.26) (0.8.1+cu101)\n","Requirement already satisfied: torch>=1.0 in /usr/local/lib/python3.6/dist-packages (from timm==0.1.26) (1.7.0+cu101)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision->timm==0.1.26) (1.19.4)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->timm==0.1.26) (7.0.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.0->timm==0.1.26) (3.7.4.3)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.0->timm==0.1.26) (0.8)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0->timm==0.1.26) (0.16.0)\n","Installing collected packages: timm\n","Successfully installed timm-0.1.26\n","Collecting albumentations==0.4.6\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/33/1c459c2c9a4028ec75527eff88bc4e2d256555189f42af4baf4d7bd89233/albumentations-0.4.6.tar.gz (117kB)\n","\u001b[K     |████████████████████████████████| 122kB 12.6MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.6) (1.19.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.6) (1.4.1)\n","Collecting imgaug>=0.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/b1/af3142c4a85cba6da9f4ebb5ff4e21e2616309552caca5e8acefe9840622/imgaug-0.4.0-py2.py3-none-any.whl (948kB)\n","\u001b[K     |████████████████████████████████| 952kB 20.7MB/s \n","\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.6) (3.13)\n","Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.6) (4.1.2.30)\n","Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (2.4.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (3.2.2)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (7.0.0)\n","Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (0.16.2)\n","Requirement already satisfied: Shapely in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.7.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.15.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (1.3.1)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.8.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (0.10.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.4.7)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2.5)\n","Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (1.1.1)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (4.4.2)\n","Building wheels for collected packages: albumentations\n","  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for albumentations: filename=albumentations-0.4.6-cp36-none-any.whl size=65164 sha256=7063705a5b86bab5e83d3fe1aa5ef8f5815d868be1a6bb5848a30042174a9bb7\n","  Stored in directory: /root/.cache/pip/wheels/c7/f4/89/56d1bee5c421c36c1a951eeb4adcc32fbb82f5344c086efa14\n","Successfully built albumentations\n","Installing collected packages: imgaug, albumentations\n","  Found existing installation: imgaug 0.2.9\n","    Uninstalling imgaug-0.2.9:\n","      Successfully uninstalled imgaug-0.2.9\n","  Found existing installation: albumentations 0.1.12\n","    Uninstalling albumentations-0.1.12:\n","      Successfully uninstalled albumentations-0.1.12\n","Successfully installed albumentations-0.4.6 imgaug-0.4.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2020-12-03T19:41:47.900849Z","iopub.status.busy":"2020-12-03T19:41:47.899850Z","iopub.status.idle":"2020-12-03T19:41:51.320552Z","shell.execute_reply":"2020-12-03T19:41:51.319742Z"},"papermill":{"duration":3.449721,"end_time":"2020-12-03T19:41:51.320687","exception":false,"start_time":"2020-12-03T19:41:47.870966","status":"completed"},"tags":[],"id":"KquhUKBI5Qu9"},"source":["import sys\n","sys.path.insert(0, \"timm-efficientdet-pytorch\")\n","sys.path.insert(0, \"omegaconf\")\n","\n","import torch\n","import os\n","from datetime import datetime\n","import time\n","import random\n","import cv2\n","import pandas as pd\n","import numpy as np\n","import albumentations as A\n","import matplotlib.pyplot as plt\n","from albumentations.pytorch.transforms import ToTensorV2\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.model_selection import GroupKFold\n","from torch.utils.data import Dataset,DataLoader\n","from torch.utils.data.sampler import SequentialSampler, RandomSampler\n","from glob import glob\n","import pandas as pd\n","from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain\n","from effdet.efficientdet import HeadNet\n","from tqdm import tqdm\n","\n","SEED = 42\n","\n","def seed_everything(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","seed_everything(SEED)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.015808,"end_time":"2020-12-03T19:41:51.352887","exception":false,"start_time":"2020-12-03T19:41:51.337079","status":"completed"},"tags":[],"id":"DVwFB2QR5Qu9"},"source":["# Data Preparation"]},{"cell_type":"code","metadata":{"id":"fUPe5jAeDhXM"},"source":["if Kaggle:\n","    BASEPATH = \"../input/nfl-impact-detection\"\n","    outdir = '.'\n","    TRAIN_IMGPATH = outdir\n","else:\n","    PATH = 'NFL/'\n","    BASEPATH = PATH + 'Data'\n","    TRAIN_IMGPATH = BASEPATH\n","    outdir = Path(PATH+'res')\n","    if not os.path.exists(outdir):\n","        os.mkdir(outdir)\n","    outdir = Path(PATH+'res/efficientDet-res')\n","    if not os.path.exists(outdir):\n","        os.mkdir(outdir)\n","    MODELNAME = \"ED5-512\"\n","    # MODELNAME = \"ED4-512\"\n","    VERSION = '{}'.format(MODELNAME)\n","    outdir = os.path.join(outdir, VERSION)\n","    if not os.path.exists(outdir):\n","        os.mkdir(outdir)\n","    # from datetime import datetime, timedelta\n","    # dateTimeObj = datetime.now()\n","    # timestampStr = dateTimeObj.strftime(\"%d-%b-%Y-%H\")\n","    # modelpath = os.path.join(outdir, 'all-kfold-hairaug-456-norm-metanew-b16')\n","    timestampStr = 'org-epoch20-aug3-gkfold'\n","    outdir = os.path.join(outdir, timestampStr)\n","    if not os.path.exists(outdir):\n","        os.mkdir(outdir)  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-12-03T19:41:51.397809Z","iopub.status.busy":"2020-12-03T19:41:51.397205Z","iopub.status.idle":"2020-12-03T19:54:33.739582Z","shell.execute_reply":"2020-12-03T19:54:33.739088Z"},"papermill":{"duration":762.371229,"end_time":"2020-12-03T19:54:33.739714","exception":false,"start_time":"2020-12-03T19:41:51.368485","status":"completed"},"tags":[],"id":"0QPqdU4t5Qu9","colab":{"base_uri":"https://localhost:8080/","height":241},"executionInfo":{"status":"ok","timestamp":1608506446985,"user_tz":480,"elapsed":49954,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}},"outputId":"f76af5be-9c90-4383-c566-4219d9371d43"},"source":["# video_labels = pd.read_csv(os.path.join(BASEPATH, 'train_labels.csv')).fillna(0)\n","# video_labels_with_impact = video_labels[video_labels['impact'] > 0]\n","# for row in tqdm(video_labels_with_impact[['video','frame','label']].values):\n","#     frames = np.array([-4,-3,-2,-1,1,2,3,4])+row[1]\n","#     video_labels.loc[(video_labels['video'] == row[0]) \n","#                                  & (video_labels['frame'].isin(frames))\n","#                                  & (video_labels['label'] == row[2]), 'impact'] = 1\n","# video_labels['image_name'] = video_labels['video'].str.replace('.mp4', '') + '_' + video_labels['frame'].astype(str) + '.png'\n","# video_labels = video_labels[video_labels.groupby('image_name')['impact'].transform(\"sum\") > 0].reset_index(drop=True)\n","# video_labels['impact'] = video_labels['impact'].astype(int)+1\n","# video_labels['x'] = video_labels['left']\n","# video_labels['y'] = video_labels['top']\n","# video_labels['w'] = video_labels['width']\n","# video_labels['h'] = video_labels['height']\n","# video_labels.head()\n","import re\n","video_labels = pd.read_csv(os.path.join(BASEPATH,'video_labels.csv'), index_col=0)\n","video_labels.loc[:,'VID'] = ''\n","video_labels.loc[:, 'VID'] =  ['_'.join(re.split('_|\\\\.', vid)[0:2]) for vid in video_labels['video']]\n","display(video_labels.head())\n","print(video_labels.shape)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>gameKey</th>\n","      <th>playID</th>\n","      <th>view</th>\n","      <th>video</th>\n","      <th>frame</th>\n","      <th>label</th>\n","      <th>left</th>\n","      <th>width</th>\n","      <th>top</th>\n","      <th>height</th>\n","      <th>impact</th>\n","      <th>impactType</th>\n","      <th>confidence</th>\n","      <th>visibility</th>\n","      <th>image_name</th>\n","      <th>x</th>\n","      <th>y</th>\n","      <th>w</th>\n","      <th>h</th>\n","      <th>VID</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>57583</td>\n","      <td>82</td>\n","      <td>Endzone</td>\n","      <td>57583_000082_Endzone.mp4</td>\n","      <td>34</td>\n","      <td>V73</td>\n","      <td>655</td>\n","      <td>21</td>\n","      <td>331</td>\n","      <td>15</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>57583_000082_Endzone_34.png</td>\n","      <td>655</td>\n","      <td>331</td>\n","      <td>21</td>\n","      <td>15</td>\n","      <td>57583_000082</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>57583</td>\n","      <td>82</td>\n","      <td>Endzone</td>\n","      <td>57583_000082_Endzone.mp4</td>\n","      <td>34</td>\n","      <td>H99</td>\n","      <td>583</td>\n","      <td>21</td>\n","      <td>312</td>\n","      <td>30</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>57583_000082_Endzone_34.png</td>\n","      <td>583</td>\n","      <td>312</td>\n","      <td>21</td>\n","      <td>30</td>\n","      <td>57583_000082</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>57583</td>\n","      <td>82</td>\n","      <td>Endzone</td>\n","      <td>57583_000082_Endzone.mp4</td>\n","      <td>34</td>\n","      <td>V15</td>\n","      <td>1069</td>\n","      <td>22</td>\n","      <td>301</td>\n","      <td>20</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>57583_000082_Endzone_34.png</td>\n","      <td>1069</td>\n","      <td>301</td>\n","      <td>22</td>\n","      <td>20</td>\n","      <td>57583_000082</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>57583</td>\n","      <td>82</td>\n","      <td>Endzone</td>\n","      <td>57583_000082_Endzone.mp4</td>\n","      <td>34</td>\n","      <td>H97</td>\n","      <td>402</td>\n","      <td>21</td>\n","      <td>313</td>\n","      <td>29</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>57583_000082_Endzone_34.png</td>\n","      <td>402</td>\n","      <td>313</td>\n","      <td>21</td>\n","      <td>29</td>\n","      <td>57583_000082</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>57583</td>\n","      <td>82</td>\n","      <td>Endzone</td>\n","      <td>57583_000082_Endzone.mp4</td>\n","      <td>34</td>\n","      <td>V72</td>\n","      <td>445</td>\n","      <td>21</td>\n","      <td>328</td>\n","      <td>16</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>57583_000082_Endzone_34.png</td>\n","      <td>445</td>\n","      <td>328</td>\n","      <td>21</td>\n","      <td>16</td>\n","      <td>57583_000082</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   gameKey  playID     view  ...   w   h           VID\n","0    57583      82  Endzone  ...  21  15  57583_000082\n","1    57583      82  Endzone  ...  21  30  57583_000082\n","2    57583      82  Endzone  ...  22  20  57583_000082\n","3    57583      82  Endzone  ...  21  29  57583_000082\n","4    57583      82  Endzone  ...  21  16  57583_000082\n","\n","[5 rows x 20 columns]"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["(197838, 20)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LB91hjvzSIcI"},"source":["# display(video_labels.head())\n","# print(video_labels.shape)\n","# video_labels.to_csv(os.path.join(BASEPATH,'video_labels.csv'), index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-12-03T19:54:35.361621Z","iopub.status.busy":"2020-12-03T19:54:35.360784Z","iopub.status.idle":"2020-12-03T19:54:35.468122Z","shell.execute_reply":"2020-12-03T19:54:35.467547Z"},"papermill":{"duration":0.925308,"end_time":"2020-12-03T19:54:35.468243","exception":false,"start_time":"2020-12-03T19:54:34.542935","status":"completed"},"tags":[],"id":"d8dFJux45Qu9"},"source":["# np.random.seed(0)\n","# video_names = np.random.permutation(video_labels.video.unique())\n","# valid_video_len = int(len(video_names)*0.2)\n","# video_valid = video_names[:valid_video_len]\n","# video_train = video_names[valid_video_len:]\n","# images_valid = video_labels[ video_labels.video.isin(video_valid)].image_name.unique()\n","# images_train = video_labels[~video_labels.video.isin(video_valid)].image_name.unique()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d1Kjkk_KuY8G"},"source":["\n","# Stratified K-Fold"]},{"cell_type":"code","metadata":{"id":"9nHFZBzzuXf3"},"source":["# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","# df_folds = video_labels[['image_name']].copy()\n","# df_folds.loc[:, 'bbox_count'] = 1\n","# df_folds = df_folds.groupby('image_name').count()\n","# df_folds.loc[:, 'video'] = video_labels[['image_name', 'video']].groupby('image_name').min()['video']\n","# df_folds.loc[:, 'stratify_group'] = np.char.add(\n","#     df_folds['video'].values.astype(str),\n","#     df_folds['bbox_count'].apply(lambda x: f'_{x // 20}').values.astype(str),\n","# )\n","\n","# df_folds.loc[:, 'fold'] = 0\n","# for fold_number, (train_index, val_index) in enumerate(skf.split(X=df_folds.index, y=df_folds['stratify_group'])):\n","#     df_folds.loc[df_folds.iloc[val_index].index, 'fold'] = fold_number"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"39waVzuQhLAh"},"source":["## Group K-Fold\n","\n","```\n","# This is formatted as code\n","```\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":467},"id":"qF7D-n-dhKHP","executionInfo":{"status":"ok","timestamp":1608506449593,"user_tz":480,"elapsed":52543,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}},"outputId":"41ac2eb3-b247-4083-eb15-989622fab93e"},"source":["gkf = GroupKFold(n_splits=5)\n","df_folds = video_labels[['image_name']].copy()\n","df_folds.loc[:, 'bbox_count'] = 1\n","df_folds = df_folds.groupby('image_name').count()\n","df_folds.loc[:, 'video'] = video_labels[['image_name', 'video']].groupby('image_name').min()['video']\n","df_folds.loc[:, 'stratify_group'] = video_labels[['image_name', 'VID']].groupby('image_name').min()['VID']\n","df_folds.loc[:, 'fold'] = 0\n","for fold_number, (train_index, val_index) in enumerate(gkf.split(X=df_folds.index, groups=df_folds['stratify_group'])):\n","    df_folds.loc[df_folds.iloc[val_index].index, 'fold'] = fold_number\n","print(display(df_folds))    "],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>bbox_count</th>\n","      <th>video</th>\n","      <th>stratify_group</th>\n","      <th>fold</th>\n","    </tr>\n","    <tr>\n","      <th>image_name</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>57583_000082_Endzone_106.png</th>\n","      <td>20</td>\n","      <td>57583_000082_Endzone.mp4</td>\n","      <td>57583_000082</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>57583_000082_Endzone_107.png</th>\n","      <td>19</td>\n","      <td>57583_000082_Endzone.mp4</td>\n","      <td>57583_000082</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>57583_000082_Endzone_108.png</th>\n","      <td>19</td>\n","      <td>57583_000082_Endzone.mp4</td>\n","      <td>57583_000082</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>57583_000082_Endzone_109.png</th>\n","      <td>20</td>\n","      <td>57583_000082_Endzone.mp4</td>\n","      <td>57583_000082</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>57583_000082_Endzone_110.png</th>\n","      <td>20</td>\n","      <td>57583_000082_Endzone.mp4</td>\n","      <td>57583_000082</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>58107_004362_Sideline_68.png</th>\n","      <td>21</td>\n","      <td>58107_004362_Sideline.mp4</td>\n","      <td>58107_004362</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>58107_004362_Sideline_69.png</th>\n","      <td>21</td>\n","      <td>58107_004362_Sideline.mp4</td>\n","      <td>58107_004362</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>58107_004362_Sideline_70.png</th>\n","      <td>21</td>\n","      <td>58107_004362_Sideline.mp4</td>\n","      <td>58107_004362</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>58107_004362_Sideline_71.png</th>\n","      <td>21</td>\n","      <td>58107_004362_Sideline.mp4</td>\n","      <td>58107_004362</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>58107_004362_Sideline_72.png</th>\n","      <td>21</td>\n","      <td>58107_004362_Sideline.mp4</td>\n","      <td>58107_004362</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>10017 rows × 4 columns</p>\n","</div>"],"text/plain":["                              bbox_count  ... fold\n","image_name                                ...     \n","57583_000082_Endzone_106.png          20  ...    1\n","57583_000082_Endzone_107.png          19  ...    1\n","57583_000082_Endzone_108.png          19  ...    1\n","57583_000082_Endzone_109.png          20  ...    1\n","57583_000082_Endzone_110.png          20  ...    1\n","...                                  ...  ...  ...\n","58107_004362_Sideline_68.png          21  ...    0\n","58107_004362_Sideline_69.png          21  ...    0\n","58107_004362_Sideline_70.png          21  ...    0\n","58107_004362_Sideline_71.png          21  ...    0\n","58107_004362_Sideline_72.png          21  ...    0\n","\n","[10017 rows x 4 columns]"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CP26_UdHiUOT","executionInfo":{"status":"ok","timestamp":1608506449593,"user_tz":480,"elapsed":52532,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}},"outputId":"62c79243-bc7b-4f2f-ad79-e6ba2c14447b"},"source":["for i in range(5):\n","    print(list(df_folds['fold'].values).count(i))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2006\n","2004\n","2006\n","2003\n","1998\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"-ffeh7KKiVHd","executionInfo":{"status":"ok","timestamp":1608506449905,"user_tz":480,"elapsed":52832,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}},"outputId":"8ff31645-683d-414a-9e06-bdbeb9136234"},"source":["video_fold_dic = pd.DataFrame(columns=['VID','fold'])\n","for vid in df_folds.stratify_group.unique():\n","    fold = df_folds.fold[df_folds.stratify_group==vid].unique()\n","    print(vid, fold)\n","    dftemp = pd.DataFrame({'VID': [vid],\n","                    'fold': [fold[0]]})\n","    video_fold_dic = video_fold_dic.append(dftemp, ignore_index= True)\n","\n","display(video_fold_dic.head())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["57583_000082 [1]\n","57584_000336 [2]\n","57584_002674 [2]\n","57586_000540 [4]\n","57586_001934 [2]\n","57586_004152 [1]\n","57594_000923 [0]\n","57596_002686 [1]\n","57597_000658 [0]\n","57597_001242 [3]\n","57676_003572 [3]\n","57679_003316 [4]\n","57680_002206 [3]\n","57680_003470 [0]\n","57682_002630 [3]\n","57684_001985 [1]\n","57686_002546 [1]\n","57700_001264 [1]\n","57775_000933 [1]\n","57778_004244 [0]\n","57781_000252 [4]\n","57782_000600 [1]\n","57783_003374 [2]\n","57784_001741 [2]\n","57785_002026 [2]\n","57786_003085 [1]\n","57787_003413 [4]\n","57788_000781 [2]\n","57790_002792 [4]\n","57790_002839 [0]\n","57904_001367 [3]\n","57905_002404 [1]\n","57906_000718 [3]\n","57907_003615 [0]\n","57910_001164 [4]\n","57911_000147 [1]\n","57911_002492 [4]\n","57912_001325 [2]\n","57913_000218 [4]\n","57915_003093 [2]\n","57992_000301 [4]\n","57992_000350 [3]\n","57993_000475 [0]\n","57995_000109 [0]\n","57997_003691 [3]\n","57998_002181 [0]\n","58000_001306 [2]\n","58005_001254 [3]\n","58005_001612 [2]\n","58048_000086 [3]\n","58093_001923 [2]\n","58094_000423 [4]\n","58094_002819 [3]\n","58095_004022 [1]\n","58098_001193 [0]\n","58102_002798 [4]\n","58103_003494 [0]\n","58104_000352 [3]\n","58106_002918 [4]\n","58107_004362 [0]\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>VID</th>\n","      <th>fold</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>57583_000082</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>57584_000336</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>57584_002674</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>57586_000540</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>57586_001934</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            VID fold\n","0  57583_000082    1\n","1  57584_000336    2\n","2  57584_002674    2\n","3  57586_000540    4\n","4  57586_001934    2"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-12-03T19:54:37.083220Z","iopub.status.busy":"2020-12-03T19:54:37.081252Z","iopub.status.idle":"2020-12-03T19:54:37.083913Z","shell.execute_reply":"2020-12-03T19:54:37.084380Z"},"papermill":{"duration":0.814812,"end_time":"2020-12-03T19:54:37.084502","exception":false,"start_time":"2020-12-03T19:54:36.269690","status":"completed"},"tags":[],"id":"JASx4ztJ5Qu-"},"source":["def mk_images(video_name, video_labels, video_dir, out_dir, only_with_impact=True):\n","    video_path=f\"{video_dir}/{video_name}\"\n","    video_name = os.path.basename(video_path)\n","    vidcap = cv2.VideoCapture(video_path)\n","    if only_with_impact:\n","        boxes_all = video_labels.query(\"video == @video_name\")\n","        print(video_path, boxes_all[boxes_all.impact > 1.0].shape[0])\n","    else:\n","        print(video_path)\n","    frame = 0\n","    while True:\n","        it_worked, img = vidcap.read()\n","        if not it_worked:\n","            break\n","        frame += 1\n","        if only_with_impact:\n","            boxes = video_labels.query(\"video == @video_name and frame == @frame\")\n","            boxes_with_impact = boxes[boxes.impact > 1.0]\n","            if boxes_with_impact.shape[0] == 0:\n","                continue\n","        img_name = f\"{video_name}_frame{frame}\"\n","        image_path = f'{out_dir}/{video_name}'.replace('.mp4',f'_{frame}.png')\n","        _ = cv2.imwrite(image_path, img)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2020-12-03T19:54:38.758603Z","iopub.status.busy":"2020-12-03T19:54:38.757834Z","iopub.status.idle":"2020-12-03T20:17:19.848931Z","shell.execute_reply":"2020-12-03T20:17:19.848179Z"},"papermill":{"duration":1361.975668,"end_time":"2020-12-03T20:17:19.849108","exception":false,"start_time":"2020-12-03T19:54:37.873440","status":"completed"},"tags":[],"id":"mF1JI58F5Qu-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608506449907,"user_tz":480,"elapsed":52820,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}},"outputId":"7bc20b8b-7634-496b-8ac7-3a99d2b025cb"},"source":["uniq_video = video_labels.video.unique()\n","# video_dir = '/kaggle/input/nfl-impact-detection/train'\n","video_dir = os.path.join(BASEPATH, 'train')\n","out_dir = os.path.join(TRAIN_IMGPATH, 'train_images')\n","print(out_dir)\n","# !mkdir -p $out_dir\n","# for video_name in uniq_video:\n","#     mk_images(video_name, video_labels, video_dir, out_dir)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["NFL/Data/train_images\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"papermill":{"duration":1.116956,"end_time":"2020-12-03T20:17:21.805986","exception":false,"start_time":"2020-12-03T20:17:20.689030","status":"completed"},"tags":[],"id":"uZDPyNTA5Qu-"},"source":["## Albumentations"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-12-03T20:17:23.531452Z","iopub.status.busy":"2020-12-03T20:17:23.529545Z","iopub.status.idle":"2020-12-03T20:17:23.535342Z","shell.execute_reply":"2020-12-03T20:17:23.534636Z"},"papermill":{"duration":0.894654,"end_time":"2020-12-03T20:17:23.535470","exception":false,"start_time":"2020-12-03T20:17:22.640816","status":"completed"},"tags":[],"id":"RBeM0iN85Qu-"},"source":["def get_train_transforms():\n","    return A.Compose(\n","        [\n","            A.HorizontalFlip(p=0.5),\n","            # A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=10, interpolation=1, border_mode=4, \n","            #                    value=None, mask_value=None, always_apply=False, p=0.5),\n","            A.OneOf([\n","              A.RandomSizedCrop(min_max_height=(500, 720), height=720, width=720, p=1.0),\n","              A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=10, interpolation=1, border_mode=4, \n","                                value=None, mask_value=None, always_apply=False, p=1.0),\n","            ], p=0.5),\n","            A.OneOf([\n","                A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, \n","                                     val_shift_limit=0.2, p=0.9),\n","                A.RandomBrightnessContrast(brightness_limit=0.2, \n","                                           contrast_limit=0.2, p=0.9),\n","            ], p=0.4),\n","            A.JpegCompression(quality_lower=85, quality_upper=95, p=0.2),\n","            A.Resize(height=512, width=512, p=1),\n","            A.OneOf([\n","                A.Blur(blur_limit=3, p=1.0),\n","                A.MedianBlur(blur_limit=3, p=1.0),\n","            ],p=0.1),\n","            A.Cutout(num_holes=8, max_h_size=64, max_w_size=64, fill_value=0, p=0.3),\n","            # A.Blur(blur_limit=5, always_apply=False, p=0.1),\n","            # A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, always_apply=False, p=0.1),\n","            ToTensorV2(p=1.0),\n","        ], \n","        p=1.0, \n","        bbox_params=A.BboxParams(\n","            format='pascal_voc',\n","            min_area=0, \n","            min_visibility=0,\n","            label_fields=['labels']\n","        )\n","    )\n","\n","def get_valid_transforms():\n","    return A.Compose(\n","        [\n","            A.Resize(height=512, width=512, p=1.0),\n","            ToTensorV2(p=1.0),\n","        ], \n","        p=1.0, \n","        bbox_params=A.BboxParams(\n","            format='pascal_voc',\n","            min_area=0, \n","            min_visibility=0,\n","            label_fields=['labels']\n","        )\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.860904,"end_time":"2020-12-03T20:17:25.233882","exception":false,"start_time":"2020-12-03T20:17:24.372978","status":"completed"},"tags":[],"id":"gYqmaqmX5Qu-"},"source":["## Dataset"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-12-03T20:17:27.167718Z","iopub.status.busy":"2020-12-03T20:17:27.166491Z","iopub.status.idle":"2020-12-03T20:17:27.179341Z","shell.execute_reply":"2020-12-03T20:17:27.178834Z"},"papermill":{"duration":1.00411,"end_time":"2020-12-03T20:17:27.179451","exception":false,"start_time":"2020-12-03T20:17:26.175341","status":"completed"},"tags":[],"id":"PpEmFOv55Qu-"},"source":["TRAIN_ROOT_PATH = os.path.join(TRAIN_IMGPATH, 'train_images')\n","\n","class DatasetRetriever(Dataset):\n","\n","    def __init__(self, marking, image_ids, transforms=None, test=False):\n","        super().__init__()\n","\n","        self.image_ids = image_ids\n","        self.marking = marking\n","        self.transforms = transforms\n","        self.test = test\n","\n","    def __getitem__(self, index: int):\n","        image_id = self.image_ids[index]\n","        \n","        # image, boxes, labels = self.load_image_and_boxes(index)\n","        if self.test or random.random() > 0.5:\n","            image, boxes, labels = self.load_image_and_boxes(index)\n","        elif random.random() > 0.33:\n","            image, boxes, labels = self.load_cutmix_image_and_boxes(index)\n","        else:\n","            image, boxes, labels = self.load_mixup_image_and_boxes(index)\n","        \n","        target = {}\n","        target['boxes'] = boxes\n","        target['labels'] = torch.tensor(labels)\n","        target['image_id'] = torch.tensor([index])\n","\n","        if self.transforms:\n","            for i in range(10):\n","                sample = self.transforms(**{\n","                    'image': image,\n","                    'bboxes': target['boxes'],\n","                    'labels': labels\n","                })\n","                if len(sample['bboxes']) > 0:\n","                    image = sample['image']\n","                    target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n","                    target['boxes'][:,[0,1,2,3]] = target['boxes'][:,[1,0,3,2]]  #yxyx: be warning\n","                    break\n","        return image, target, image_id\n","\n","    def __len__(self) -> int:\n","        return self.image_ids.shape[0]\n","\n","    def load_image_and_boxes(self, index):\n","        image_id = self.image_ids[index]\n","        # print(f'{TRAIN_ROOT_PATH}/{image_id}')\n","        image = cv2.imread(f'{TRAIN_ROOT_PATH}/{image_id}', cv2.IMREAD_COLOR).copy().astype(np.float32)\n","        # img = cv2.imread(f'{TRAIN_ROOT_PATH}/{image_id}', cv2.IMREAD_COLOR)\n","        # if img is NoneType:\n","        #     print(f'{TRAIN_ROOT_PATH}/{image_id}')\n","        # image = img.copy().astype(np.float32)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n","        image /= 255.0\n","        records = self.marking[self.marking['image_name'] == image_id]\n","        boxes = records[['x', 'y', 'w', 'h']].values\n","        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n","        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n","        labels = records['impact'].values\n","        return image, boxes, labels\n","\n","    def load_mixup_image_and_boxes(self, index):\n","        image, boxes, labels = self.load_image_and_boxes(index)\n","        r_image, r_boxes, r_labels = self.load_image_and_boxes(random.randint(0, self.image_ids.shape[0] - 1))\n","        return (image+r_image)/2, np.vstack((boxes, r_boxes)).astype(np.int32), np.concatenate((labels, r_labels))\n","    \n","\n","    def load_cutmix_image_and_boxes(self, index, imsize=720):\n","        \"\"\" \n","        This implementation of cutmix author:  https://www.kaggle.com/nvnnghia \n","        Refactoring and adaptation: https://www.kaggle.com/shonenkov\n","        \"\"\"\n","        w, h = imsize, imsize\n","        s = imsize // 2\n","    \n","        xc, yc = [int(random.uniform(imsize * 0.25, imsize * 0.75)) for _ in range(2)]  # center x, y\n","        indexes = [index] + [random.randint(0, self.image_ids.shape[0] - 1) for _ in range(3)]\n","\n","        result_image = np.full((imsize, imsize, 3), 1, dtype=np.float32)\n","        result_boxes = []\n","        result_labels = np.array([], dtype=np.int)\n","\n","        for i, index in enumerate(indexes):\n","            image, boxes, labels = self.load_image_and_boxes(index)\n","            if i == 0:\n","                x1a, y1a, x2a, y2a = max(xc - w, 0), max(yc - h, 0), xc, yc  # xmin, ymin, xmax, ymax (large image)\n","                x1b, y1b, x2b, y2b = w - (x2a - x1a), h - (y2a - y1a), w, h  # xmin, ymin, xmax, ymax (small image)\n","            elif i == 1:  # top right\n","                x1a, y1a, x2a, y2a = xc, max(yc - h, 0), min(xc + w, s * 2), yc\n","                x1b, y1b, x2b, y2b = 0, h - (y2a - y1a), min(w, x2a - x1a), h\n","            elif i == 2:  # bottom left\n","                x1a, y1a, x2a, y2a = max(xc - w, 0), yc, xc, min(s * 2, yc + h)\n","                x1b, y1b, x2b, y2b = w - (x2a - x1a), 0, max(xc, w), min(y2a - y1a, h)\n","            elif i == 3:  # bottom right\n","                x1a, y1a, x2a, y2a = xc, yc, min(xc + w, s * 2), min(s * 2, yc + h)\n","                x1b, y1b, x2b, y2b = 0, 0, min(w, x2a - x1a), min(y2a - y1a, h)\n","            result_image[y1a:y2a, x1a:x2a] = image[y1b:y2b, x1b:x2b]\n","            padw = x1a - x1b\n","            padh = y1a - y1b\n","\n","            boxes[:, 0] += padw\n","            boxes[:, 1] += padh\n","            boxes[:, 2] += padw\n","            boxes[:, 3] += padh\n","\n","            result_boxes.append(boxes)\n","            result_labels = np.concatenate((result_labels, labels))\n","\n","        result_boxes = np.concatenate(result_boxes, 0)\n","        np.clip(result_boxes[:, 0:], 0, 2 * s, out=result_boxes[:, 0:])\n","        result_boxes = result_boxes.astype(np.int32)\n","        index_to_use = np.where((result_boxes[:,2]-result_boxes[:,0])*(result_boxes[:,3]-result_boxes[:,1]) > 0)\n","        result_boxes = result_boxes[index_to_use]\n","        result_labels = result_labels[index_to_use]\n","        \n","        return result_image, result_boxes, result_labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-12-03T20:17:29.786515Z","iopub.status.busy":"2020-12-03T20:17:29.785545Z","iopub.status.idle":"2020-12-03T20:17:29.787381Z","shell.execute_reply":"2020-12-03T20:17:29.788067Z"},"papermill":{"duration":1.299025,"end_time":"2020-12-03T20:17:29.788236","exception":false,"start_time":"2020-12-03T20:17:28.489211","status":"completed"},"tags":[],"id":"zfBRK8395Qu-"},"source":["# train_dataset = DatasetRetriever(\n","#     # image_ids=images_train,\n","#     image_ids=df_folds[df_folds['fold'] != fold_number].index.values,\n","#     marking=video_labels,\n","#     transforms=get_train_transforms(),\n","#     test=False,\n","# )\n","\n","# validation_dataset = DatasetRetriever(\n","#     # image_ids=images_valid,\n","#     image_ids=df_folds[df_folds['fold'] == fold_number].index.values,\n","#     marking=video_labels,\n","#     transforms=get_valid_transforms(),\n","#     test=True,\n","# )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.854518,"end_time":"2020-12-03T20:17:31.516845","exception":false,"start_time":"2020-12-03T20:17:30.662327","status":"completed"},"tags":[],"id":"fR6RQZ_B5Qu-"},"source":["## Fitter"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-12-03T20:17:33.240203Z","iopub.status.busy":"2020-12-03T20:17:33.238373Z","iopub.status.idle":"2020-12-03T20:17:33.240858Z","shell.execute_reply":"2020-12-03T20:17:33.241329Z"},"papermill":{"duration":0.857125,"end_time":"2020-12-03T20:17:33.241450","exception":false,"start_time":"2020-12-03T20:17:32.384325","status":"completed"},"tags":[],"id":"2Ds6QgCh5Qu-"},"source":["class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-12-03T20:17:34.928263Z","iopub.status.busy":"2020-12-03T20:17:34.893824Z","iopub.status.idle":"2020-12-03T20:17:34.943197Z","shell.execute_reply":"2020-12-03T20:17:34.942605Z"},"papermill":{"duration":0.879671,"end_time":"2020-12-03T20:17:34.943328","exception":false,"start_time":"2020-12-03T20:17:34.063657","status":"completed"},"tags":[],"id":"pY_EQeAl5Qu-"},"source":["import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","class Fitter:\n","    \n","    def __init__(self, model, device, config, checkpointfile, train_on_checkpoint):\n","        self.config = config\n","        self.epoch = 0\n","\n","        self.base_dir = f'./{config.folder}'\n","        if not os.path.exists(self.base_dir):\n","            os.makedirs(self.base_dir)\n","        \n","        self.log_path = f'{self.base_dir}/log.txt'\n","        self.best_summary_loss = 10**5\n","\n","        self.model = model\n","        self.device = device\n","\n","        param_optimizer = list(self.model.named_parameters())\n","        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","        optimizer_grouped_parameters = [\n","            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n","            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","        ] \n","\n","        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=config.lr)\n","        self.scheduler = config.SchedulerClass(self.optimizer, **config.scheduler_params)\n","        self.log(f'Fitter prepared. Device is {self.device}')\n","        self.checkpointfile = checkpointfile\n","        self.train_on_checkpoint = train_on_checkpoint\n","\n","    def fit(self, train_loader, validation_loader):\n","        if self.train_on_checkpoint:\n","            self.load(self.checkpointfile)\n","            print(\"Loading from pretrained-model: {}\".format(self.checkpointfile))\n","        for e in range(self.config.n_epochs):\n","            if self.config.verbose:\n","                lr = self.optimizer.param_groups[0]['lr']\n","                timestamp = datetime.utcnow().isoformat()\n","                self.log(f'\\n{timestamp}\\nLR: {lr}')\n","\n","            t = time.time()\n","            summary_loss = self.train_one_epoch(train_loader)\n","\n","            self.log(f'[RESULT]: Train. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, time: {(time.time() - t):.5f}')\n","            self.save(f'{self.base_dir}/last-checkpoint.bin')\n","\n","            t = time.time()\n","            summary_loss = self.validation(validation_loader)\n","\n","            self.log(f'[RESULT]: Val. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, time: {(time.time() - t):.5f}')\n","            if summary_loss.avg < self.best_summary_loss:\n","                self.best_summary_loss = summary_loss.avg\n","                self.model.eval()\n","                self.save(f'{self.base_dir}/best-checkpoint-{str(self.epoch).zfill(3)}epoch.bin')\n","                for path in sorted(glob(f'{self.base_dir}/best-checkpoint-*epoch.bin'))[:-3]:\n","                    os.remove(path)\n","\n","            if self.config.validation_scheduler:\n","                self.scheduler.step(metrics=summary_loss.avg)\n","\n","            self.epoch += 1\n","\n","    def validation(self, val_loader):\n","        self.model.eval()\n","        summary_loss = AverageMeter()\n","        t = time.time()\n","        for step, (images, targets, image_ids) in enumerate(val_loader):\n","            if self.config.verbose:\n","                if step % self.config.verbose_step == 0:\n","                    print(\n","                        f'Val Step {step}/{len(val_loader)}, ' + \\\n","                        f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n","                        f'time: {(time.time() - t):.5f}', end='\\r'\n","                    )\n","            with torch.no_grad():\n","                images = torch.stack(images)\n","                batch_size = images.shape[0]\n","                images = images.to(self.device).float()\n","                boxes = [target['boxes'].to(self.device).float() for target in targets]\n","                labels = [target['labels'].to(self.device).float() for target in targets]\n","\n","                loss, _, _ = self.model(images, boxes, labels)\n","                summary_loss.update(loss.detach().item(), batch_size)\n","\n","        return summary_loss\n","\n","    def train_one_epoch(self, train_loader):\n","        self.model.train()\n","        summary_loss = AverageMeter()\n","        t = time.time()\n","        for step, (images, targets, image_ids) in enumerate(train_loader):\n","            if self.config.verbose:\n","                if step % self.config.verbose_step == 0:\n","                    print(\n","                        f'Train Step {step}/{len(train_loader)}, ' + \\\n","                        f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n","                        f'time: {(time.time() - t):.5f}', end='\\r'\n","                    )\n","            \n","            images = torch.stack(images)\n","            images = images.to(self.device).float()\n","            batch_size = images.shape[0]\n","            boxes = [target['boxes'].to(self.device).float() for target in targets]\n","            labels = [target['labels'].to(self.device).float() for target in targets]\n","\n","            self.optimizer.zero_grad()\n","            \n","            loss, _, _ = self.model(images, boxes, labels)\n","            \n","            loss.backward()\n","\n","            summary_loss.update(loss.detach().item(), batch_size)\n","\n","            self.optimizer.step()\n","\n","            if self.config.step_scheduler:\n","                self.scheduler.step()\n","\n","        return summary_loss\n","    \n","    def save(self, path):\n","        self.model.eval()\n","        torch.save({\n","            'model_state_dict': self.model.model.state_dict(),\n","            'optimizer_state_dict': self.optimizer.state_dict(),\n","            'scheduler_state_dict': self.scheduler.state_dict(),\n","            'best_summary_loss': self.best_summary_loss,\n","            'epoch': self.epoch,\n","        }, path)\n","\n","    def load(self, path):\n","        checkpoint = torch.load(path)\n","        self.model.model.load_state_dict(checkpoint['model_state_dict'])\n","        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n","        self.best_summary_loss = checkpoint['best_summary_loss']\n","        self.epoch = checkpoint['epoch'] + 1\n","        \n","    def log(self, message):\n","        if self.config.verbose:\n","            print(message)\n","        with open(self.log_path, 'a+') as logger:\n","            logger.write(f'{message}\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-12-03T20:17:36.640201Z","iopub.status.busy":"2020-12-03T20:17:36.639432Z","iopub.status.idle":"2020-12-03T20:17:36.643624Z","shell.execute_reply":"2020-12-03T20:17:36.643063Z"},"papermill":{"duration":0.840689,"end_time":"2020-12-03T20:17:36.643746","exception":false,"start_time":"2020-12-03T20:17:35.803057","status":"completed"},"tags":[],"id":"Qu5pIEq85Qu-"},"source":["class TrainGlobalConfig:\n","    num_workers = 4\n","    batch_size = 4 \n","    n_epochs = 6\n","    lr = 0.0002\n","    folder = os.path.join(outdir, 'effdet5-models')\n","    verbose = True\n","    verbose_step = 1\n","    step_scheduler = False\n","    validation_scheduler = True\n","    SchedulerClass = torch.optim.lr_scheduler.ReduceLROnPlateau\n","    scheduler_params = dict(\n","        mode='min',\n","        factor=0.5,\n","        patience=1,\n","        verbose=False, \n","        threshold=0.0001,\n","        threshold_mode='abs',\n","        cooldown=0, \n","        min_lr=1e-8,\n","        eps=1e-08\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NB_9UvPwPU9e"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-12-03T20:17:38.345119Z","iopub.status.busy":"2020-12-03T20:17:38.344232Z","iopub.status.idle":"2020-12-03T20:17:38.347364Z","shell.execute_reply":"2020-12-03T20:17:38.346780Z"},"papermill":{"duration":0.856628,"end_time":"2020-12-03T20:17:38.347461","exception":false,"start_time":"2020-12-03T20:17:37.490833","status":"completed"},"tags":[],"id":"3pDRCaNk5Qu-"},"source":["def collate_fn(batch):\n","    return tuple(zip(*batch))\n","\n","# def run_training():\n","#     if Kaggle:\n","#         device = torch.device('cuda:0')\n","#     else:\n","#         device = ('cuda' if torch.cuda.is_available() else 'cpu')\n","#     net.to(device)\n","\n","#     train_loader = torch.utils.data.DataLoader(\n","#         train_dataset,\n","#         batch_size=TrainGlobalConfig.batch_size,\n","#         sampler=RandomSampler(train_dataset),\n","#         pin_memory=False,\n","#         drop_last=True,\n","#         num_workers=TrainGlobalConfig.num_workers,\n","#         collate_fn=collate_fn,\n","#     )\n","#     val_loader = torch.utils.data.DataLoader(\n","#         validation_dataset, \n","#         batch_size=TrainGlobalConfig.batch_size,\n","#         num_workers=TrainGlobalConfig.num_workers,\n","#         shuffle=False,\n","#         sampler=SequentialSampler(validation_dataset),\n","#         pin_memory=False,\n","#         collate_fn=collate_fn,\n","#     )\n","\n","#     fitter = Fitter(model=net, device=device, config=TrainGlobalConfig)\n","#     fitter.fit(train_loader, val_loader)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SqIytc00zi-2"},"source":["def run_training_kfold(num_fold):\n","    # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    #device = torch.device('cuda:0')\n","    TRAIN_ON_CHECKPOINT = True\n","    if Kaggle:\n","        device = torch.device('cuda:0')\n","    else:\n","        device = ('cuda' if torch.cuda.is_available() else 'cpu')\n","    for fold_number in range(num_fold):\n","        print('Fold: {}'.format(fold_number + 1))\n","        if fold_number<1:\n","            continue\n","        # if fold_number==3:\n","        #     TRAIN_ON_CHECKPOINT = True\n","        # else:\n","        #     TRAIN_ON_CHECKPOINT = False\n","        print(\"TRAIN_ON_CHECKPOINT:\", TRAIN_ON_CHECKPOINT)\n","        train_dataset = DatasetRetriever(\n","            image_ids=df_folds[df_folds['fold'] != fold_number].index.values,\n","            marking=video_labels,\n","            transforms=get_train_transforms(),\n","            test=False,\n","        )\n","\n","        validation_dataset = DatasetRetriever(\n","            image_ids=df_folds[df_folds['fold'] == fold_number].index.values,\n","            marking=video_labels,\n","            transforms=get_valid_transforms(),\n","            test=True,\n","        )\n","        # print(\"preparing dataset done!\")\n","        train_loader = torch.utils.data.DataLoader(\n","            train_dataset,\n","            batch_size=TrainGlobalConfig.batch_size,\n","            sampler=RandomSampler(train_dataset),\n","            pin_memory=False,\n","            drop_last=True,\n","            num_workers=TrainGlobalConfig.num_workers,\n","            collate_fn=collate_fn,\n","        )\n","        val_loader = torch.utils.data.DataLoader(\n","            validation_dataset, \n","            batch_size=TrainGlobalConfig.batch_size,\n","            num_workers=TrainGlobalConfig.num_workers,\n","            shuffle=False,\n","            sampler=SequentialSampler(validation_dataset),\n","            pin_memory=False,\n","            collate_fn=collate_fn,\n","        )\n","        # print(\"preparing loader done\")\n","        net, checkpoint_file = get_net(fold_number, TRAIN_ON_CHECKPOINT)\n","        net.to(device)\n","        TrainGlobalConfig.folder = os.path.join(outdir, f'effdet5-models/fold{fold_number}')\n","        fitter = Fitter(model=net, device=device, config=TrainGlobalConfig, checkpointfile=checkpoint_file, train_on_checkpoint = TRAIN_ON_CHECKPOINT)\n","        fitter.fit(train_loader, val_loader)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-12-03T20:17:40.490346Z","iopub.status.busy":"2020-12-03T20:17:40.480631Z","iopub.status.idle":"2020-12-03T20:17:44.356599Z","shell.execute_reply":"2020-12-03T20:17:44.355423Z"},"papermill":{"duration":4.767308,"end_time":"2020-12-03T20:17:44.356749","exception":false,"start_time":"2020-12-03T20:17:39.589441","status":"completed"},"tags":[],"id":"nVTdclME5Qu-"},"source":["def get_net(fold_number, train_on_checkpoint):\n","    # config = get_efficientdet_config('tf_efficientdet_d5')\n","    # print(\"Enter get_net, TRAIN_ON_CHECKPOINT:\", train_on_checkpoint)\n","    config = get_efficientdet_config('tf_efficientdet_d5')\n","\n","    net = EfficientDet(config, pretrained_backbone=False)\n","\n","    if train_on_checkpoint:\n","        # print(outdir)\n","        checkpointfile = os.path.join(outdir, f'effdet5-models/fold{fold_number}/last-checkpoint.bin')\n","    else:\n","        checkpoint = torch.load(os.path.join(PATH, 'nfl-models/efficientdet_d5-ef44aea8.pth'))\n","        net.load_state_dict(checkpoint)\n","        checkpointfile = ''\n","    print(\"checkpointfile:\", checkpointfile)\n","    # checkpoint = torch.load(os.path.join(PATH, 'nfl-models/efficientdet_d5-ef44aea8.pth'))\n","    # checkpoint = torch.load(os.path.join(PATH, 'nfl-models/efficientdet_d4-5b370b7a.pth'))\n","    # net.load_state_dict(checkpoint)\n","    config.num_classes = 2\n","    config.image_size = 512\n","    net.class_net = HeadNet(config, num_outputs=config.num_classes, norm_kwargs=dict(eps=.001, momentum=.01))\n","    return DetBenchTrain(net, config), checkpointfile\n","\n","# net, checkpointfile = get_net()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-12-03T20:17:46.495960Z","iopub.status.busy":"2020-12-03T20:17:46.069898Z","iopub.status.idle":"2020-12-03T21:15:33.319215Z","shell.execute_reply":"2020-12-03T21:15:33.318628Z"},"papermill":{"duration":3468.111783,"end_time":"2020-12-03T21:15:33.319336","exception":false,"start_time":"2020-12-03T20:17:45.207553","status":"completed"},"scrolled":true,"tags":[],"id":"lNN08JJW5Qu-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608539534119,"user_tz":480,"elapsed":3754377,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}},"outputId":"28559bd5-2662-4fcb-aaee-d6ad2e0bf219"},"source":["# run_training()\n","run_training_kfold(num_fold=5)"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Fold: 1\n","Fold: 2\n","TRAIN_ON_CHECKPOINT: True\n","checkpointfile: NFL/res/efficientDet-res/ED5-512/org-epoch20-aug3-gkfold/effdet5-models/fold1/last-checkpoint.bin\n","Fitter prepared. Device is cuda\n","Loading from pretrained-model: NFL/res/efficientDet-res/ED5-512/org-epoch20-aug3-gkfold/effdet5-models/fold1/last-checkpoint.bin\n","\n","2020-12-20T23:21:13.587572\n","LR: 0.0002\n","[RESULT]: Train. Epoch: 15, summary_loss: 0.47796, time: 2136.11632\n","[RESULT]: Val. Epoch: 15, summary_loss: 0.41456, time: 439.20427\n","\n","2020-12-21T00:04:10.899115\n","LR: 0.0001\n","[RESULT]: Train. Epoch: 16, summary_loss: 0.44265, time: 1198.93355\n","[RESULT]: Val. Epoch: 16, summary_loss: 0.40725, time: 89.29586\n","\n","2020-12-21T00:25:41.039275\n","LR: 0.0001\n","[RESULT]: Train. Epoch: 17, summary_loss: 0.43334, time: 1175.87627\n","[RESULT]: Val. Epoch: 17, summary_loss: 0.41203, time: 86.56687\n","\n","2020-12-21T00:46:45.456722\n","LR: 5e-05\n","[RESULT]: Train. Epoch: 18, summary_loss: 0.42150, time: 1195.83626\n","[RESULT]: Val. Epoch: 18, summary_loss: 0.40688, time: 92.52326\n","\n","2020-12-21T01:08:15.891450\n","LR: 5e-05\n","[RESULT]: Train. Epoch: 19, summary_loss: 0.40966, time: 1218.65281\n","[RESULT]: Val. Epoch: 19, summary_loss: 0.41280, time: 88.25288\n","\n","2020-12-21T01:30:04.686089\n","LR: 2.5e-05\n","[RESULT]: Train. Epoch: 20, summary_loss: 0.39917, time: 1198.67109\n","[RESULT]: Val. Epoch: 20, summary_loss: 0.40879, time: 90.94604\n","Fold: 3\n","TRAIN_ON_CHECKPOINT: True\n","checkpointfile: NFL/res/efficientDet-res/ED5-512/org-epoch20-aug3-gkfold/effdet5-models/fold2/last-checkpoint.bin\n","Fitter prepared. Device is cuda\n","Loading from pretrained-model: NFL/res/efficientDet-res/ED5-512/org-epoch20-aug3-gkfold/effdet5-models/fold2/last-checkpoint.bin\n","\n","2020-12-21T01:51:48.135221\n","LR: 5e-05\n","[RESULT]: Train. Epoch: 16, summary_loss: 0.42516, time: 1218.01985\n","[RESULT]: Val. Epoch: 16, summary_loss: 0.39116, time: 90.62592\n","\n","2020-12-21T02:13:39.103970\n","LR: 5e-05\n","[RESULT]: Train. Epoch: 17, summary_loss: 0.41928, time: 1208.57531\n","[RESULT]: Val. Epoch: 17, summary_loss: 0.39055, time: 90.87813\n","\n","2020-12-21T02:35:20.398260\n","LR: 2.5e-05\n","[RESULT]: Train. Epoch: 18, summary_loss: 0.40610, time: 1207.40643\n","[RESULT]: Val. Epoch: 18, summary_loss: 0.40122, time: 91.30995\n","\n","2020-12-21T02:57:01.102134\n","LR: 2.5e-05\n","[RESULT]: Train. Epoch: 19, summary_loss: 0.41255, time: 1214.84329\n","[RESULT]: Val. Epoch: 19, summary_loss: 0.40253, time: 94.50323\n","\n","2020-12-21T03:18:52.519566\n","LR: 1.25e-05\n","[RESULT]: Train. Epoch: 20, summary_loss: 0.40845, time: 1230.74253\n","[RESULT]: Val. Epoch: 20, summary_loss: 0.39687, time: 93.98876\n","\n","2020-12-21T03:40:59.264501\n","LR: 1.25e-05\n","[RESULT]: Train. Epoch: 21, summary_loss: 0.40718, time: 1228.60927\n","[RESULT]: Val. Epoch: 21, summary_loss: 0.40116, time: 91.49743\n","Fold: 4\n","TRAIN_ON_CHECKPOINT: True\n","checkpointfile: NFL/res/efficientDet-res/ED5-512/org-epoch20-aug3-gkfold/effdet5-models/fold3/last-checkpoint.bin\n","Fitter prepared. Device is cuda\n","Loading from pretrained-model: NFL/res/efficientDet-res/ED5-512/org-epoch20-aug3-gkfold/effdet5-models/fold3/last-checkpoint.bin\n","\n","2020-12-21T04:03:14.950261\n","LR: 0.0001\n","[RESULT]: Train. Epoch: 20, summary_loss: 0.41787, time: 1228.14852\n","[RESULT]: Val. Epoch: 20, summary_loss: 0.44385, time: 91.49114\n","\n","2020-12-21T04:25:16.798409\n","LR: 0.0001\n","[RESULT]: Train. Epoch: 21, summary_loss: 0.41488, time: 1227.07357\n","[RESULT]: Val. Epoch: 21, summary_loss: 0.43197, time: 91.46867\n","\n","2020-12-21T04:47:17.459120\n","LR: 5e-05\n","[RESULT]: Train. Epoch: 22, summary_loss: 0.39679, time: 1218.21928\n","[RESULT]: Val. Epoch: 22, summary_loss: 0.43692, time: 93.68603\n","\n","2020-12-21T05:09:11.212330\n","LR: 5e-05\n","[RESULT]: Train. Epoch: 23, summary_loss: 0.38810, time: 1223.58277\n","[RESULT]: Val. Epoch: 23, summary_loss: 0.44991, time: 91.91849\n","\n","2020-12-21T05:31:08.739974\n","LR: 2.5e-05\n","[RESULT]: Train. Epoch: 24, summary_loss: 0.37506, time: 1230.58269\n","[RESULT]: Val. Epoch: 24, summary_loss: 0.45430, time: 91.90392\n","\n","2020-12-21T05:53:13.232070\n","LR: 2.5e-05\n","[RESULT]: Train. Epoch: 25, summary_loss: 0.38109, time: 1230.15004\n","[RESULT]: Val. Epoch: 25, summary_loss: 0.45323, time: 94.84812\n","Fold: 5\n","TRAIN_ON_CHECKPOINT: True\n","checkpointfile: NFL/res/efficientDet-res/ED5-512/org-epoch20-aug3-gkfold/effdet5-models/fold4/last-checkpoint.bin\n","Fitter prepared. Device is cuda\n","Loading from pretrained-model: NFL/res/efficientDet-res/ED5-512/org-epoch20-aug3-gkfold/effdet5-models/fold4/last-checkpoint.bin\n","\n","2020-12-21T06:15:33.602377\n","LR: 0.0001\n","[RESULT]: Train. Epoch: 16, summary_loss: 0.45712, time: 1262.90409\n","[RESULT]: Val. Epoch: 16, summary_loss: 0.38724, time: 96.08244\n","\n","2020-12-21T06:38:14.748603\n","LR: 0.0001\n","[RESULT]: Train. Epoch: 17, summary_loss: 0.45409, time: 1267.77057\n","[RESULT]: Val. Epoch: 17, summary_loss: 0.38743, time: 95.95720\n","\n","2020-12-21T07:01:00.662619\n","LR: 5e-05\n","[RESULT]: Train. Epoch: 18, summary_loss: 0.44214, time: 1267.53907\n","[RESULT]: Val. Epoch: 18, summary_loss: 0.37869, time: 96.33077\n","\n","2020-12-21T07:23:49.429515\n","LR: 5e-05\n","[RESULT]: Train. Epoch: 19, summary_loss: 0.43302, time: 1275.31948\n","[RESULT]: Val. Epoch: 19, summary_loss: 0.38378, time: 96.17898\n","\n","2020-12-21T07:46:43.002703\n","LR: 5e-05\n","[RESULT]: Train. Epoch: 20, summary_loss: 0.43339, time: 1269.52136\n","[RESULT]: Val. Epoch: 20, summary_loss: 0.38150, time: 95.91830\n","\n","2020-12-21T08:09:30.579467\n","LR: 2.5e-05\n","[RESULT]: Train. Epoch: 21, summary_loss: 0.42393, time: 1268.25148\n","[RESULT]: Val. Epoch: 21, summary_loss: 0.38202, time: 96.14296\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-12-03T21:15:39.110558Z","iopub.status.busy":"2020-12-03T21:15:39.109713Z","iopub.status.idle":"2020-12-03T21:15:40.239972Z","shell.execute_reply":"2020-12-03T21:15:40.239159Z"},"papermill":{"duration":3.984352,"end_time":"2020-12-03T21:15:40.240095","exception":false,"start_time":"2020-12-03T21:15:36.255743","status":"completed"},"tags":[],"id":"TduA6fs65Qu-","executionInfo":{"status":"ok","timestamp":1608539534120,"user_tz":480,"elapsed":20,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}}},"source":["# clearing working dir\n","# be careful when running this code on local environment!\n","# !rm -rf *\n","# !mv * /tmp/train_images\n","\n","# import shutil\n","# shutil.rmtree(out_dir)  "],"execution_count":25,"outputs":[]}]}