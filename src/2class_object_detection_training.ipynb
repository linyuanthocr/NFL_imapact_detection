{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "papermill": {
      "duration": 5672.030464,
      "end_time": "2020-12-03T21:15:43.903691",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2020-12-03T19:41:11.873227",
      "version": "2.1.0"
    },
    "colab": {
      "name": "2class-object-detection-training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.016133,
          "end_time": "2020-12-03T19:41:16.277625",
          "exception": false,
          "start_time": "2020-12-03T19:41:16.261492",
          "status": "completed"
        },
        "tags": [],
        "id": "SbAt3SlE5Qu8"
      },
      "source": [
        "This notebook detects 2 class objects.\n",
        "- class1: helmet without impact\n",
        "- class2: helmet with impact\n",
        "\n",
        "Object Detection part is based on [EfficientDet notebook](https://www.kaggle.com/shonenkov/training-efficientdet) for [global wheat detection competition](https://www.kaggle.com/c/global-wheat-detection) by [shonenkov](https://www.kaggle.com/shonenkov), which is using [github repos efficientdet-pytorch](https://github.com/rwightman/efficientdet-pytorch) by [@rwightman](https://www.kaggle.com/rwightman).\n",
        "\n",
        "Inference part can be foud [here](https://www.kaggle.com/its7171/2class-object-detection-inference/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkvTg6_c6Cj0"
      },
      "source": [
        "Kaggle = False\n",
        "Colab = !Kaggle"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmIZ8aCDBp6-",
        "outputId": "4678d70f-7425-417e-913c-28c8661b0f16"
      },
      "source": [
        "import os, sys\n",
        "from pathlib import Path\n",
        "\n",
        "if Colab:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    path = \"/content/drive/My Drive\"\n",
        "    os.chdir(path)\n",
        "    os.listdir(path)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "execution": {
          "iopub.execute_input": "2020-12-03T19:41:16.315908Z",
          "iopub.status.busy": "2020-12-03T19:41:16.314887Z",
          "iopub.status.idle": "2020-12-03T19:41:47.853592Z",
          "shell.execute_reply": "2020-12-03T19:41:47.852000Z"
        },
        "papermill": {
          "duration": 31.561437,
          "end_time": "2020-12-03T19:41:47.853762",
          "exception": false,
          "start_time": "2020-12-03T19:41:16.292325",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b33SLkXj5Qu8",
        "outputId": "f3c9fcf0-18b9-42b9-d60c-6544cdd718b6"
      },
      "source": [
        "if Kaggle:\n",
        "    !pip install ../input/nfl-lib/timm-0.1.26-py3-none-any.whl\n",
        "    !tar xfz ../input/nfl-lib/pkgs.tgz\n",
        "else:\n",
        "    !pip install NFL/nfl-lib/timm-0.1.26-py3-none-any.whl\n",
        "    !tar xfz NFL/nfl-lib/pkgs.tgz    \n",
        "    !pip install albumentations==0.4.6\n",
        "# for pytorch1.6\n",
        "cmd = \"sed -i -e 's/ \\/ / \\/\\/ /' timm-efficientdet-pytorch/effdet/bench.py\"\n",
        "!$cmd"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing ./NFL/nfl-lib/timm-0.1.26-py3-none-any.whl\n",
            "Requirement already satisfied: torch>=1.0 in /usr/local/lib/python3.6/dist-packages (from timm==0.1.26) (1.7.0+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from timm==0.1.26) (0.8.1+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0->timm==0.1.26) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.0->timm==0.1.26) (0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.0->timm==0.1.26) (1.18.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.0->timm==0.1.26) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->timm==0.1.26) (7.0.0)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.1.26\n",
            "Collecting albumentations==0.4.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/33/1c459c2c9a4028ec75527eff88bc4e2d256555189f42af4baf4d7bd89233/albumentations-0.4.6.tar.gz (117kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 5.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.6) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.6) (1.4.1)\n",
            "Collecting imgaug>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/b1/af3142c4a85cba6da9f4ebb5ff4e21e2616309552caca5e8acefe9840622/imgaug-0.4.0-py2.py3-none-any.whl (948kB)\n",
            "\u001b[K     |████████████████████████████████| 952kB 18.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.6) (3.13)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.6) (4.1.2.30)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (0.16.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (3.2.2)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (2.4.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (7.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.15.0)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.7.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2.5)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (1.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.4.7)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (4.4.2)\n",
            "Building wheels for collected packages: albumentations\n",
            "  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for albumentations: filename=albumentations-0.4.6-cp36-none-any.whl size=65165 sha256=a4ada8700506022abe4364c810e9f35f2f66bef7e83925b309f870aa2fc0d04e\n",
            "  Stored in directory: /root/.cache/pip/wheels/c7/f4/89/56d1bee5c421c36c1a951eeb4adcc32fbb82f5344c086efa14\n",
            "Successfully built albumentations\n",
            "Installing collected packages: imgaug, albumentations\n",
            "  Found existing installation: imgaug 0.2.9\n",
            "    Uninstalling imgaug-0.2.9:\n",
            "      Successfully uninstalled imgaug-0.2.9\n",
            "  Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed albumentations-0.4.6 imgaug-0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "execution": {
          "iopub.execute_input": "2020-12-03T19:41:47.900849Z",
          "iopub.status.busy": "2020-12-03T19:41:47.899850Z",
          "iopub.status.idle": "2020-12-03T19:41:51.320552Z",
          "shell.execute_reply": "2020-12-03T19:41:51.319742Z"
        },
        "papermill": {
          "duration": 3.449721,
          "end_time": "2020-12-03T19:41:51.320687",
          "exception": false,
          "start_time": "2020-12-03T19:41:47.870966",
          "status": "completed"
        },
        "tags": [],
        "id": "KquhUKBI5Qu9"
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, \"timm-efficientdet-pytorch\")\n",
        "sys.path.insert(0, \"omegaconf\")\n",
        "\n",
        "import torch\n",
        "import os\n",
        "from datetime import datetime\n",
        "import time\n",
        "import random\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import albumentations as A\n",
        "import matplotlib.pyplot as plt\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
        "from glob import glob\n",
        "import pandas as pd\n",
        "from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain\n",
        "from effdet.efficientdet import HeadNet\n",
        "from tqdm import tqdm\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(SEED)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.015808,
          "end_time": "2020-12-03T19:41:51.352887",
          "exception": false,
          "start_time": "2020-12-03T19:41:51.337079",
          "status": "completed"
        },
        "tags": [],
        "id": "DVwFB2QR5Qu9"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUPe5jAeDhXM"
      },
      "source": [
        "if Kaggle:\n",
        "    BASEPATH = \"../input/nfl-impact-detection\"\n",
        "    outdir = '.'\n",
        "    TRAIN_IMGPATH = outdir\n",
        "else:\n",
        "    PATH = 'NFL/'\n",
        "    BASEPATH = PATH + 'Data'\n",
        "    TRAIN_IMGPATH = BASEPATH\n",
        "    outdir = Path(PATH+'res')\n",
        "    if not os.path.exists(outdir):\n",
        "        os.mkdir(outdir)\n",
        "    outdir = Path(PATH+'res/efficientDet-res')\n",
        "    if not os.path.exists(outdir):\n",
        "        os.mkdir(outdir)\n",
        "    MODELNAME = \"ED5-512\"\n",
        "    VERSION = '{}'.format(MODELNAME)\n",
        "    outdir = os.path.join(outdir, VERSION)\n",
        "    if not os.path.exists(outdir):\n",
        "        os.mkdir(outdir)\n",
        "    # from datetime import datetime, timedelta\n",
        "    # dateTimeObj = datetime.now()\n",
        "    # timestampStr = dateTimeObj.strftime(\"%d-%b-%Y-%H\")\n",
        "    # modelpath = os.path.join(outdir, 'all-kfold-hairaug-456-norm-metanew-b16')\n",
        "    timestampStr = 'org-epoch20-aug1'\n",
        "    outdir = os.path.join(outdir, timestampStr)\n",
        "    if not os.path.exists(outdir):\n",
        "        os.mkdir(outdir)  "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-12-03T19:41:51.397809Z",
          "iopub.status.busy": "2020-12-03T19:41:51.397205Z",
          "iopub.status.idle": "2020-12-03T19:54:33.739582Z",
          "shell.execute_reply": "2020-12-03T19:54:33.739088Z"
        },
        "papermill": {
          "duration": 762.371229,
          "end_time": "2020-12-03T19:54:33.739714",
          "exception": false,
          "start_time": "2020-12-03T19:41:51.368485",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "0QPqdU4t5Qu9",
        "outputId": "0ff6e895-f568-474e-c9d9-4da0a067570a"
      },
      "source": [
        "# video_labels = pd.read_csv(os.path.join(BASEPATH, 'train_labels.csv')).fillna(0)\n",
        "# video_labels_with_impact = video_labels[video_labels['impact'] > 0]\n",
        "# for row in tqdm(video_labels_with_impact[['video','frame','label']].values):\n",
        "#     frames = np.array([-4,-3,-2,-1,1,2,3,4])+row[1]\n",
        "#     video_labels.loc[(video_labels['video'] == row[0]) \n",
        "#                                  & (video_labels['frame'].isin(frames))\n",
        "#                                  & (video_labels['label'] == row[2]), 'impact'] = 1\n",
        "# video_labels['image_name'] = video_labels['video'].str.replace('.mp4', '') + '_' + video_labels['frame'].astype(str) + '.png'\n",
        "# video_labels = video_labels[video_labels.groupby('image_name')['impact'].transform(\"sum\") > 0].reset_index(drop=True)\n",
        "# video_labels['impact'] = video_labels['impact'].astype(int)+1\n",
        "# video_labels['x'] = video_labels['left']\n",
        "# video_labels['y'] = video_labels['top']\n",
        "# video_labels['w'] = video_labels['width']\n",
        "# video_labels['h'] = video_labels['height']\n",
        "# video_labels.head()\n",
        "\n",
        "video_labels = pd.read_csv(os.path.join(BASEPATH,'video_labels.csv'), index_col=0)\n",
        "display(video_labels.head())\n",
        "print(video_labels.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gameKey</th>\n",
              "      <th>playID</th>\n",
              "      <th>view</th>\n",
              "      <th>video</th>\n",
              "      <th>frame</th>\n",
              "      <th>label</th>\n",
              "      <th>left</th>\n",
              "      <th>width</th>\n",
              "      <th>top</th>\n",
              "      <th>height</th>\n",
              "      <th>impact</th>\n",
              "      <th>impactType</th>\n",
              "      <th>confidence</th>\n",
              "      <th>visibility</th>\n",
              "      <th>image_name</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>w</th>\n",
              "      <th>h</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>57583</td>\n",
              "      <td>82</td>\n",
              "      <td>Endzone</td>\n",
              "      <td>57583_000082_Endzone.mp4</td>\n",
              "      <td>34</td>\n",
              "      <td>V73</td>\n",
              "      <td>655</td>\n",
              "      <td>21</td>\n",
              "      <td>331</td>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>57583_000082_Endzone_34.png</td>\n",
              "      <td>655</td>\n",
              "      <td>331</td>\n",
              "      <td>21</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>57583</td>\n",
              "      <td>82</td>\n",
              "      <td>Endzone</td>\n",
              "      <td>57583_000082_Endzone.mp4</td>\n",
              "      <td>34</td>\n",
              "      <td>H99</td>\n",
              "      <td>583</td>\n",
              "      <td>21</td>\n",
              "      <td>312</td>\n",
              "      <td>30</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>57583_000082_Endzone_34.png</td>\n",
              "      <td>583</td>\n",
              "      <td>312</td>\n",
              "      <td>21</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>57583</td>\n",
              "      <td>82</td>\n",
              "      <td>Endzone</td>\n",
              "      <td>57583_000082_Endzone.mp4</td>\n",
              "      <td>34</td>\n",
              "      <td>V15</td>\n",
              "      <td>1069</td>\n",
              "      <td>22</td>\n",
              "      <td>301</td>\n",
              "      <td>20</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>57583_000082_Endzone_34.png</td>\n",
              "      <td>1069</td>\n",
              "      <td>301</td>\n",
              "      <td>22</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>57583</td>\n",
              "      <td>82</td>\n",
              "      <td>Endzone</td>\n",
              "      <td>57583_000082_Endzone.mp4</td>\n",
              "      <td>34</td>\n",
              "      <td>H97</td>\n",
              "      <td>402</td>\n",
              "      <td>21</td>\n",
              "      <td>313</td>\n",
              "      <td>29</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>57583_000082_Endzone_34.png</td>\n",
              "      <td>402</td>\n",
              "      <td>313</td>\n",
              "      <td>21</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>57583</td>\n",
              "      <td>82</td>\n",
              "      <td>Endzone</td>\n",
              "      <td>57583_000082_Endzone.mp4</td>\n",
              "      <td>34</td>\n",
              "      <td>V72</td>\n",
              "      <td>445</td>\n",
              "      <td>21</td>\n",
              "      <td>328</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>57583_000082_Endzone_34.png</td>\n",
              "      <td>445</td>\n",
              "      <td>328</td>\n",
              "      <td>21</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   gameKey  playID     view                     video  ...     x    y   w   h\n",
              "0    57583      82  Endzone  57583_000082_Endzone.mp4  ...   655  331  21  15\n",
              "1    57583      82  Endzone  57583_000082_Endzone.mp4  ...   583  312  21  30\n",
              "2    57583      82  Endzone  57583_000082_Endzone.mp4  ...  1069  301  22  20\n",
              "3    57583      82  Endzone  57583_000082_Endzone.mp4  ...   402  313  21  29\n",
              "4    57583      82  Endzone  57583_000082_Endzone.mp4  ...   445  328  21  16\n",
              "\n",
              "[5 rows x 19 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "(197838, 19)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LB91hjvzSIcI"
      },
      "source": [
        "# display(video_labels.head())\n",
        "# print(video_labels.shape)\n",
        "# video_labels.to_csv(os.path.join(BASEPATH,'video_labels.csv'), index=False)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-12-03T19:54:35.361621Z",
          "iopub.status.busy": "2020-12-03T19:54:35.360784Z",
          "iopub.status.idle": "2020-12-03T19:54:35.468122Z",
          "shell.execute_reply": "2020-12-03T19:54:35.467547Z"
        },
        "papermill": {
          "duration": 0.925308,
          "end_time": "2020-12-03T19:54:35.468243",
          "exception": false,
          "start_time": "2020-12-03T19:54:34.542935",
          "status": "completed"
        },
        "tags": [],
        "id": "d8dFJux45Qu9"
      },
      "source": [
        "np.random.seed(0)\n",
        "video_names = np.random.permutation(video_labels.video.unique())\n",
        "valid_video_len = int(len(video_names)*0.2)\n",
        "video_valid = video_names[:valid_video_len]\n",
        "video_train = video_names[valid_video_len:]\n",
        "images_valid = video_labels[ video_labels.video.isin(video_valid)].image_name.unique()\n",
        "images_train = video_labels[~video_labels.video.isin(video_valid)].image_name.unique()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-12-03T19:54:37.083220Z",
          "iopub.status.busy": "2020-12-03T19:54:37.081252Z",
          "iopub.status.idle": "2020-12-03T19:54:37.083913Z",
          "shell.execute_reply": "2020-12-03T19:54:37.084380Z"
        },
        "papermill": {
          "duration": 0.814812,
          "end_time": "2020-12-03T19:54:37.084502",
          "exception": false,
          "start_time": "2020-12-03T19:54:36.269690",
          "status": "completed"
        },
        "tags": [],
        "id": "JASx4ztJ5Qu-"
      },
      "source": [
        "def mk_images(video_name, video_labels, video_dir, out_dir, only_with_impact=True):\n",
        "    video_path=f\"{video_dir}/{video_name}\"\n",
        "    video_name = os.path.basename(video_path)\n",
        "    vidcap = cv2.VideoCapture(video_path)\n",
        "    if only_with_impact:\n",
        "        boxes_all = video_labels.query(\"video == @video_name\")\n",
        "        print(video_path, boxes_all[boxes_all.impact > 1.0].shape[0])\n",
        "    else:\n",
        "        print(video_path)\n",
        "    frame = 0\n",
        "    while True:\n",
        "        it_worked, img = vidcap.read()\n",
        "        if not it_worked:\n",
        "            break\n",
        "        frame += 1\n",
        "        if only_with_impact:\n",
        "            boxes = video_labels.query(\"video == @video_name and frame == @frame\")\n",
        "            boxes_with_impact = boxes[boxes.impact > 1.0]\n",
        "            if boxes_with_impact.shape[0] == 0:\n",
        "                continue\n",
        "        img_name = f\"{video_name}_frame{frame}\"\n",
        "        image_path = f'{out_dir}/{video_name}'.replace('.mp4',f'_{frame}.png')\n",
        "        _ = cv2.imwrite(image_path, img)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.execute_input": "2020-12-03T19:54:38.758603Z",
          "iopub.status.busy": "2020-12-03T19:54:38.757834Z",
          "iopub.status.idle": "2020-12-03T20:17:19.848931Z",
          "shell.execute_reply": "2020-12-03T20:17:19.848179Z"
        },
        "papermill": {
          "duration": 1361.975668,
          "end_time": "2020-12-03T20:17:19.849108",
          "exception": false,
          "start_time": "2020-12-03T19:54:37.873440",
          "status": "completed"
        },
        "tags": [],
        "id": "mF1JI58F5Qu-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d43a5d7d-3ab3-4b7f-e32a-817c64bf7e2e"
      },
      "source": [
        "uniq_video = video_labels.video.unique()\n",
        "# video_dir = '/kaggle/input/nfl-impact-detection/train'\n",
        "video_dir = os.path.join(BASEPATH, 'train')\n",
        "out_dir = os.path.join(TRAIN_IMGPATH, 'train_images')\n",
        "print(out_dir)\n",
        "# !mkdir -p $out_dir\n",
        "# for video_name in uniq_video:\n",
        "#     mk_images(video_name, video_labels, video_dir, out_dir)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NFL/Data/train_images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 1.116956,
          "end_time": "2020-12-03T20:17:21.805986",
          "exception": false,
          "start_time": "2020-12-03T20:17:20.689030",
          "status": "completed"
        },
        "tags": [],
        "id": "uZDPyNTA5Qu-"
      },
      "source": [
        "## Albumentations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-12-03T20:17:23.531452Z",
          "iopub.status.busy": "2020-12-03T20:17:23.529545Z",
          "iopub.status.idle": "2020-12-03T20:17:23.535342Z",
          "shell.execute_reply": "2020-12-03T20:17:23.534636Z"
        },
        "papermill": {
          "duration": 0.894654,
          "end_time": "2020-12-03T20:17:23.535470",
          "exception": false,
          "start_time": "2020-12-03T20:17:22.640816",
          "status": "completed"
        },
        "tags": [],
        "id": "RBeM0iN85Qu-"
      },
      "source": [
        "def get_train_transforms():\n",
        "    return A.Compose(\n",
        "        [\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=10, interpolation=1, border_mode=4, \n",
        "                               value=None, mask_value=None, always_apply=False, p=0.5),\n",
        "            A.Resize(height=512, width=512, p=1),\n",
        "            A.Blur(blur_limit=5, always_apply=False, p=0.1),\n",
        "            A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, always_apply=False, p=0.1),\n",
        "            ToTensorV2(p=1.0),\n",
        "        ], \n",
        "        p=1.0, \n",
        "        bbox_params=A.BboxParams(\n",
        "            format='pascal_voc',\n",
        "            min_area=0, \n",
        "            min_visibility=0,\n",
        "            label_fields=['labels']\n",
        "        )\n",
        "    )\n",
        "\n",
        "def get_valid_transforms():\n",
        "    return A.Compose(\n",
        "        [\n",
        "            A.Resize(height=512, width=512, p=1.0),\n",
        "            ToTensorV2(p=1.0),\n",
        "        ], \n",
        "        p=1.0, \n",
        "        bbox_params=A.BboxParams(\n",
        "            format='pascal_voc',\n",
        "            min_area=0, \n",
        "            min_visibility=0,\n",
        "            label_fields=['labels']\n",
        "        )\n",
        "    )"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.860904,
          "end_time": "2020-12-03T20:17:25.233882",
          "exception": false,
          "start_time": "2020-12-03T20:17:24.372978",
          "status": "completed"
        },
        "tags": [],
        "id": "gYqmaqmX5Qu-"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-12-03T20:17:27.167718Z",
          "iopub.status.busy": "2020-12-03T20:17:27.166491Z",
          "iopub.status.idle": "2020-12-03T20:17:27.179341Z",
          "shell.execute_reply": "2020-12-03T20:17:27.178834Z"
        },
        "papermill": {
          "duration": 1.00411,
          "end_time": "2020-12-03T20:17:27.179451",
          "exception": false,
          "start_time": "2020-12-03T20:17:26.175341",
          "status": "completed"
        },
        "tags": [],
        "id": "PpEmFOv55Qu-"
      },
      "source": [
        "TRAIN_ROOT_PATH = os.path.join(TRAIN_IMGPATH, 'train_images')\n",
        "\n",
        "class DatasetRetriever(Dataset):\n",
        "\n",
        "    def __init__(self, marking, image_ids, transforms=None, test=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.image_ids = image_ids\n",
        "        self.marking = marking\n",
        "        self.transforms = transforms\n",
        "        self.test = test\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        image_id = self.image_ids[index]\n",
        "        \n",
        "        image, boxes, labels = self.load_image_and_boxes(index)\n",
        "        \n",
        "        target = {}\n",
        "        target['boxes'] = boxes\n",
        "        target['labels'] = torch.tensor(labels)\n",
        "        target['image_id'] = torch.tensor([index])\n",
        "\n",
        "        if self.transforms:\n",
        "            for i in range(10):\n",
        "                sample = self.transforms(**{\n",
        "                    'image': image,\n",
        "                    'bboxes': target['boxes'],\n",
        "                    'labels': labels\n",
        "                })\n",
        "                if len(sample['bboxes']) > 0:\n",
        "                    image = sample['image']\n",
        "                    target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
        "                    target['boxes'][:,[0,1,2,3]] = target['boxes'][:,[1,0,3,2]]  #yxyx: be warning\n",
        "                    break\n",
        "        return image, target, image_id\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.image_ids.shape[0]\n",
        "\n",
        "    def load_image_and_boxes(self, index):\n",
        "        image_id = self.image_ids[index]\n",
        "        # print(f'{TRAIN_ROOT_PATH}/{image_id}')\n",
        "        image = cv2.imread(f'{TRAIN_ROOT_PATH}/{image_id}', cv2.IMREAD_COLOR).copy().astype(np.float32)\n",
        "        # img = cv2.imread(f'{TRAIN_ROOT_PATH}/{image_id}', cv2.IMREAD_COLOR)\n",
        "        # if img is NoneType:\n",
        "        #     print(f'{TRAIN_ROOT_PATH}/{image_id}')\n",
        "        # image = img.copy().astype(np.float32)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "        image /= 255.0\n",
        "        records = self.marking[self.marking['image_name'] == image_id]\n",
        "        boxes = records[['x', 'y', 'w', 'h']].values\n",
        "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
        "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
        "        labels = records['impact'].values\n",
        "        return image, boxes, labels"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-12-03T20:17:29.786515Z",
          "iopub.status.busy": "2020-12-03T20:17:29.785545Z",
          "iopub.status.idle": "2020-12-03T20:17:29.787381Z",
          "shell.execute_reply": "2020-12-03T20:17:29.788067Z"
        },
        "papermill": {
          "duration": 1.299025,
          "end_time": "2020-12-03T20:17:29.788236",
          "exception": false,
          "start_time": "2020-12-03T20:17:28.489211",
          "status": "completed"
        },
        "tags": [],
        "id": "zfBRK8395Qu-"
      },
      "source": [
        "train_dataset = DatasetRetriever(\n",
        "    image_ids=images_train,\n",
        "    marking=video_labels,\n",
        "    transforms=get_train_transforms(),\n",
        "    test=False,\n",
        ")\n",
        "\n",
        "validation_dataset = DatasetRetriever(\n",
        "    image_ids=images_valid,\n",
        "    marking=video_labels,\n",
        "    transforms=get_valid_transforms(),\n",
        "    test=True,\n",
        ")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.854518,
          "end_time": "2020-12-03T20:17:31.516845",
          "exception": false,
          "start_time": "2020-12-03T20:17:30.662327",
          "status": "completed"
        },
        "tags": [],
        "id": "fR6RQZ_B5Qu-"
      },
      "source": [
        "## Fitter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-12-03T20:17:33.240203Z",
          "iopub.status.busy": "2020-12-03T20:17:33.238373Z",
          "iopub.status.idle": "2020-12-03T20:17:33.240858Z",
          "shell.execute_reply": "2020-12-03T20:17:33.241329Z"
        },
        "papermill": {
          "duration": 0.857125,
          "end_time": "2020-12-03T20:17:33.241450",
          "exception": false,
          "start_time": "2020-12-03T20:17:32.384325",
          "status": "completed"
        },
        "tags": [],
        "id": "2Ds6QgCh5Qu-"
      },
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-12-03T20:17:34.928263Z",
          "iopub.status.busy": "2020-12-03T20:17:34.893824Z",
          "iopub.status.idle": "2020-12-03T20:17:34.943197Z",
          "shell.execute_reply": "2020-12-03T20:17:34.942605Z"
        },
        "papermill": {
          "duration": 0.879671,
          "end_time": "2020-12-03T20:17:34.943328",
          "exception": false,
          "start_time": "2020-12-03T20:17:34.063657",
          "status": "completed"
        },
        "tags": [],
        "id": "pY_EQeAl5Qu-"
      },
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "class Fitter:\n",
        "    \n",
        "    def __init__(self, model, device, config):\n",
        "        self.config = config\n",
        "        self.epoch = 0\n",
        "\n",
        "        self.base_dir = f'./{config.folder}'\n",
        "        if not os.path.exists(self.base_dir):\n",
        "            os.makedirs(self.base_dir)\n",
        "        \n",
        "        self.log_path = f'{self.base_dir}/log.txt'\n",
        "        self.best_summary_loss = 10**5\n",
        "\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "\n",
        "        param_optimizer = list(self.model.named_parameters())\n",
        "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "        optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
        "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ] \n",
        "\n",
        "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=config.lr)\n",
        "        self.scheduler = config.SchedulerClass(self.optimizer, **config.scheduler_params)\n",
        "        self.log(f'Fitter prepared. Device is {self.device}')\n",
        "\n",
        "    def fit(self, train_loader, validation_loader):\n",
        "        for e in range(self.config.n_epochs):\n",
        "            if self.config.verbose:\n",
        "                lr = self.optimizer.param_groups[0]['lr']\n",
        "                timestamp = datetime.utcnow().isoformat()\n",
        "                self.log(f'\\n{timestamp}\\nLR: {lr}')\n",
        "\n",
        "            t = time.time()\n",
        "            summary_loss = self.train_one_epoch(train_loader)\n",
        "\n",
        "            self.log(f'[RESULT]: Train. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, time: {(time.time() - t):.5f}')\n",
        "            self.save(f'{self.base_dir}/last-checkpoint.bin')\n",
        "\n",
        "            t = time.time()\n",
        "            summary_loss = self.validation(validation_loader)\n",
        "\n",
        "            self.log(f'[RESULT]: Val. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, time: {(time.time() - t):.5f}')\n",
        "            if summary_loss.avg < self.best_summary_loss:\n",
        "                self.best_summary_loss = summary_loss.avg\n",
        "                self.model.eval()\n",
        "                self.save(f'{self.base_dir}/best-checkpoint-{str(self.epoch).zfill(3)}epoch.bin')\n",
        "                for path in sorted(glob(f'{self.base_dir}/best-checkpoint-*epoch.bin'))[:-3]:\n",
        "                    os.remove(path)\n",
        "\n",
        "            if self.config.validation_scheduler:\n",
        "                self.scheduler.step(metrics=summary_loss.avg)\n",
        "\n",
        "            self.epoch += 1\n",
        "\n",
        "    def validation(self, val_loader):\n",
        "        self.model.eval()\n",
        "        summary_loss = AverageMeter()\n",
        "        t = time.time()\n",
        "        for step, (images, targets, image_ids) in enumerate(val_loader):\n",
        "            if self.config.verbose:\n",
        "                if step % self.config.verbose_step == 0:\n",
        "                    print(\n",
        "                        f'Val Step {step}/{len(val_loader)}, ' + \\\n",
        "                        f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n",
        "                        f'time: {(time.time() - t):.5f}', end='\\r'\n",
        "                    )\n",
        "            with torch.no_grad():\n",
        "                images = torch.stack(images)\n",
        "                batch_size = images.shape[0]\n",
        "                images = images.to(self.device).float()\n",
        "                boxes = [target['boxes'].to(self.device).float() for target in targets]\n",
        "                labels = [target['labels'].to(self.device).float() for target in targets]\n",
        "\n",
        "                loss, _, _ = self.model(images, boxes, labels)\n",
        "                summary_loss.update(loss.detach().item(), batch_size)\n",
        "\n",
        "        return summary_loss\n",
        "\n",
        "    def train_one_epoch(self, train_loader):\n",
        "        self.model.train()\n",
        "        summary_loss = AverageMeter()\n",
        "        t = time.time()\n",
        "        for step, (images, targets, image_ids) in enumerate(train_loader):\n",
        "            if self.config.verbose:\n",
        "                if step % self.config.verbose_step == 0:\n",
        "                    print(\n",
        "                        f'Train Step {step}/{len(train_loader)}, ' + \\\n",
        "                        f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n",
        "                        f'time: {(time.time() - t):.5f}', end='\\r'\n",
        "                    )\n",
        "            \n",
        "            images = torch.stack(images)\n",
        "            images = images.to(self.device).float()\n",
        "            batch_size = images.shape[0]\n",
        "            boxes = [target['boxes'].to(self.device).float() for target in targets]\n",
        "            labels = [target['labels'].to(self.device).float() for target in targets]\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            \n",
        "            loss, _, _ = self.model(images, boxes, labels)\n",
        "            \n",
        "            loss.backward()\n",
        "\n",
        "            summary_loss.update(loss.detach().item(), batch_size)\n",
        "\n",
        "            self.optimizer.step()\n",
        "\n",
        "            if self.config.step_scheduler:\n",
        "                self.scheduler.step()\n",
        "\n",
        "        return summary_loss\n",
        "    \n",
        "    def save(self, path):\n",
        "        self.model.eval()\n",
        "        torch.save({\n",
        "            'model_state_dict': self.model.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
        "            'best_summary_loss': self.best_summary_loss,\n",
        "            'epoch': self.epoch,\n",
        "        }, path)\n",
        "\n",
        "    def load(self, path):\n",
        "        checkpoint = torch.load(path)\n",
        "        self.model.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "        self.best_summary_loss = checkpoint['best_summary_loss']\n",
        "        self.epoch = checkpoint['epoch'] + 1\n",
        "        \n",
        "    def log(self, message):\n",
        "        if self.config.verbose:\n",
        "            print(message)\n",
        "        with open(self.log_path, 'a+') as logger:\n",
        "            logger.write(f'{message}\\n')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-12-03T20:17:36.640201Z",
          "iopub.status.busy": "2020-12-03T20:17:36.639432Z",
          "iopub.status.idle": "2020-12-03T20:17:36.643624Z",
          "shell.execute_reply": "2020-12-03T20:17:36.643063Z"
        },
        "papermill": {
          "duration": 0.840689,
          "end_time": "2020-12-03T20:17:36.643746",
          "exception": false,
          "start_time": "2020-12-03T20:17:35.803057",
          "status": "completed"
        },
        "tags": [],
        "id": "Qu5pIEq85Qu-"
      },
      "source": [
        "class TrainGlobalConfig:\n",
        "    num_workers = 4\n",
        "    batch_size = 4 \n",
        "    n_epochs = 20\n",
        "    lr = 0.0002\n",
        "    folder = os.path.join(outdir, 'effdet5-models')\n",
        "    verbose = True\n",
        "    verbose_step = 1\n",
        "    step_scheduler = False\n",
        "    validation_scheduler = True\n",
        "    SchedulerClass = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
        "    scheduler_params = dict(\n",
        "        mode='min',\n",
        "        factor=0.5,\n",
        "        patience=1,\n",
        "        verbose=False, \n",
        "        threshold=0.0001,\n",
        "        threshold_mode='abs',\n",
        "        cooldown=0, \n",
        "        min_lr=1e-8,\n",
        "        eps=1e-08\n",
        "    )"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-12-03T20:17:38.345119Z",
          "iopub.status.busy": "2020-12-03T20:17:38.344232Z",
          "iopub.status.idle": "2020-12-03T20:17:38.347364Z",
          "shell.execute_reply": "2020-12-03T20:17:38.346780Z"
        },
        "papermill": {
          "duration": 0.856628,
          "end_time": "2020-12-03T20:17:38.347461",
          "exception": false,
          "start_time": "2020-12-03T20:17:37.490833",
          "status": "completed"
        },
        "tags": [],
        "id": "3pDRCaNk5Qu-"
      },
      "source": [
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "def run_training():\n",
        "    if Kaggle:\n",
        "        device = torch.device('cuda:0')\n",
        "    else:\n",
        "        device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    net.to(device)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=TrainGlobalConfig.batch_size,\n",
        "        sampler=RandomSampler(train_dataset),\n",
        "        pin_memory=False,\n",
        "        drop_last=True,\n",
        "        num_workers=TrainGlobalConfig.num_workers,\n",
        "        collate_fn=collate_fn,\n",
        "    )\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        validation_dataset, \n",
        "        batch_size=TrainGlobalConfig.batch_size,\n",
        "        num_workers=TrainGlobalConfig.num_workers,\n",
        "        shuffle=False,\n",
        "        sampler=SequentialSampler(validation_dataset),\n",
        "        pin_memory=False,\n",
        "        collate_fn=collate_fn,\n",
        "    )\n",
        "\n",
        "    fitter = Fitter(model=net, device=device, config=TrainGlobalConfig)\n",
        "    fitter.fit(train_loader, val_loader)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-12-03T20:17:40.490346Z",
          "iopub.status.busy": "2020-12-03T20:17:40.480631Z",
          "iopub.status.idle": "2020-12-03T20:17:44.356599Z",
          "shell.execute_reply": "2020-12-03T20:17:44.355423Z"
        },
        "papermill": {
          "duration": 4.767308,
          "end_time": "2020-12-03T20:17:44.356749",
          "exception": false,
          "start_time": "2020-12-03T20:17:39.589441",
          "status": "completed"
        },
        "tags": [],
        "id": "nVTdclME5Qu-"
      },
      "source": [
        "def get_net():\n",
        "    config = get_efficientdet_config('tf_efficientdet_d5')\n",
        "    net = EfficientDet(config, pretrained_backbone=False)\n",
        "    checkpoint = torch.load(os.path.join(PATH, 'nfl-models/efficientdet_d5-ef44aea8.pth'))\n",
        "    net.load_state_dict(checkpoint)\n",
        "    config.num_classes = 2\n",
        "    config.image_size = 512\n",
        "    net.class_net = HeadNet(config, num_outputs=config.num_classes, norm_kwargs=dict(eps=.001, momentum=.01))\n",
        "    return DetBenchTrain(net, config)\n",
        "\n",
        "net = get_net()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-12-03T20:17:46.495960Z",
          "iopub.status.busy": "2020-12-03T20:17:46.069898Z",
          "iopub.status.idle": "2020-12-03T21:15:33.319215Z",
          "shell.execute_reply": "2020-12-03T21:15:33.318628Z"
        },
        "papermill": {
          "duration": 3468.111783,
          "end_time": "2020-12-03T21:15:33.319336",
          "exception": false,
          "start_time": "2020-12-03T20:17:45.207553",
          "status": "completed"
        },
        "scrolled": true,
        "tags": [],
        "id": "lNN08JJW5Qu-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90d2afe5-919a-4244-f521-c498e17ddfab"
      },
      "source": [
        "run_training()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitter prepared. Device is cuda\n",
            "\n",
            "2020-12-05T18:51:18.491533\n",
            "LR: 0.0002\n",
            "[RESULT]: Train. Epoch: 0, summary_loss: 34.96576, time: 3210.38497\n",
            "[RESULT]: Val. Epoch: 0, summary_loss: 0.75771, time: 748.97816\n",
            "\n",
            "2020-12-05T19:57:22.463525\n",
            "LR: 0.0002\n",
            "[RESULT]: Train. Epoch: 1, summary_loss: 0.68814, time: 1870.60920\n",
            "[RESULT]: Val. Epoch: 1, summary_loss: 0.58253, time: 110.45883\n",
            "\n",
            "2020-12-05T20:30:29.274830\n",
            "LR: 0.0002\n",
            "[RESULT]: Train. Epoch: 2, summary_loss: 0.55769, time: 1882.27487\n",
            "[RESULT]: Val. Epoch: 2, summary_loss: 0.48443, time: 109.43748\n",
            "\n",
            "2020-12-05T21:03:45.121714\n",
            "LR: 0.0002\n",
            "[RESULT]: Train. Epoch: 3, summary_loss: 0.50383, time: 1862.00347\n",
            "[RESULT]: Val. Epoch: 3, summary_loss: 0.47818, time: 105.88258\n",
            "\n",
            "2020-12-05T21:36:37.177517\n",
            "LR: 0.0002\n",
            "[RESULT]: Train. Epoch: 4, summary_loss: 0.47522, time: 1853.74262\n",
            "[RESULT]: Val. Epoch: 4, summary_loss: 0.44413, time: 105.49370\n",
            "\n",
            "2020-12-05T22:09:20.855155\n",
            "LR: 0.0002\n",
            "[RESULT]: Train. Epoch: 5, summary_loss: 0.45104, time: 1831.70156\n",
            "[RESULT]: Val. Epoch: 5, summary_loss: 0.46237, time: 102.64167\n",
            "\n",
            "2020-12-05T22:41:37.319360\n",
            "LR: 0.0002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-12-03T21:15:39.110558Z",
          "iopub.status.busy": "2020-12-03T21:15:39.109713Z",
          "iopub.status.idle": "2020-12-03T21:15:40.239972Z",
          "shell.execute_reply": "2020-12-03T21:15:40.239159Z"
        },
        "papermill": {
          "duration": 3.984352,
          "end_time": "2020-12-03T21:15:40.240095",
          "exception": false,
          "start_time": "2020-12-03T21:15:36.255743",
          "status": "completed"
        },
        "tags": [],
        "id": "TduA6fs65Qu-"
      },
      "source": [
        "# clearing working dir\n",
        "# be careful when running this code on local environment!\n",
        "# !rm -rf *\n",
        "# !mv * /tmp/train_images\n",
        "\n",
        "# import shutil\n",
        "# shutil.rmtree(out_dir)  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}