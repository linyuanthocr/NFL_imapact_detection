{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"papermill":{"duration":5672.030464,"end_time":"2020-12-03T21:15:43.903691","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2020-12-03T19:41:11.873227","version":"2.1.0"},"colab":{"name":"2class-object-detection-training-kfold-cutmix-OOF-ed2.ipynb","provenance":[{"file_id":"1hekQQEdhAtf042ATqI5Y859gudARY0Ct","timestamp":1607286130579}],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"papermill":{"duration":0.016133,"end_time":"2020-12-03T19:41:16.277625","exception":false,"start_time":"2020-12-03T19:41:16.261492","status":"completed"},"tags":[],"id":"SbAt3SlE5Qu8"},"source":["This notebook detects 2 class objects.\n","- class1: helmet without impact\n","- class2: helmet with impact\n","\n","Object Detection part is based on [EfficientDet notebook](https://www.kaggle.com/shonenkov/training-efficientdet) for [global wheat detection competition](https://www.kaggle.com/c/global-wheat-detection) by [shonenkov](https://www.kaggle.com/shonenkov), which is using [github repos efficientdet-pytorch](https://github.com/rwightman/efficientdet-pytorch) by [@rwightman](https://www.kaggle.com/rwightman).\n","\n","Inference part can be foud [here](https://www.kaggle.com/its7171/2class-object-detection-inference/)."]},{"cell_type":"code","metadata":{"id":"FkvTg6_c6Cj0","executionInfo":{"status":"ok","timestamp":1608744576076,"user_tz":480,"elapsed":832,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}}},"source":["Kaggle = False\n","Colab = not Kaggle\n","TRAIN = False"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"JmIZ8aCDBp6-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608744577068,"user_tz":480,"elapsed":1811,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}},"outputId":"0fe0b188-a3f2-475a-d7f8-7a029dce381e"},"source":["import os, sys\n","from pathlib import Path\n","\n","if Colab:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","\n","    path = \"/content/drive/My Drive\"\n","    os.chdir(path)\n","    os.listdir(path)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2020-12-03T19:41:16.315908Z","iopub.status.busy":"2020-12-03T19:41:16.314887Z","iopub.status.idle":"2020-12-03T19:41:47.853592Z","shell.execute_reply":"2020-12-03T19:41:47.852000Z"},"papermill":{"duration":31.561437,"end_time":"2020-12-03T19:41:47.853762","exception":false,"start_time":"2020-12-03T19:41:16.292325","status":"completed"},"tags":[],"id":"b33SLkXj5Qu8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608744583142,"user_tz":480,"elapsed":7873,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}},"outputId":"b602de7c-740f-4bbc-b67a-7b131d388c19"},"source":["if Kaggle:\n","    !pip install ../input/nfl-lib/timm-0.1.26-py3-none-any.whl\n","    !tar xfz ../input/nfl-lib/pkgs.tgz\n","else:\n","    !pip install NFL/nfl-lib/timm-0.1.26-py3-none-any.whl\n","    !tar xfz NFL/nfl-lib/pkgs.tgz    \n","    !pip install albumentations==0.4.6\n","# for pytorch1.6\n","cmd = \"sed -i -e 's/ \\/ / \\/\\/ /' timm-efficientdet-pytorch/effdet/bench.py\"\n","!$cmd"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: timm==0.1.26 from file:///content/drive/My%20Drive/NFL/nfl-lib/timm-0.1.26-py3-none-any.whl in /usr/local/lib/python3.6/dist-packages (0.1.26)\n","Requirement already satisfied: torch>=1.0 in /usr/local/lib/python3.6/dist-packages (from timm==0.1.26) (1.7.0+cu101)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from timm==0.1.26) (0.8.1+cu101)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0->timm==0.1.26) (0.16.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.0->timm==0.1.26) (3.7.4.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.0->timm==0.1.26) (1.19.4)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.0->timm==0.1.26) (0.8)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->timm==0.1.26) (7.0.0)\n","Requirement already satisfied: albumentations==0.4.6 in /usr/local/lib/python3.6/dist-packages (0.4.6)\n","Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.6) (4.1.2.30)\n","Requirement already satisfied: imgaug>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.6) (0.4.0)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.6) (3.13)\n","Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.6) (1.19.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.6) (1.4.1)\n","Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (0.16.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (3.2.2)\n","Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (2.4.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.15.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (7.0.0)\n","Requirement already satisfied: Shapely in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.7.1)\n","Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (1.1.1)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2.5)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.4.7)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.8.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (1.3.1)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (4.4.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2020-12-03T19:41:47.900849Z","iopub.status.busy":"2020-12-03T19:41:47.899850Z","iopub.status.idle":"2020-12-03T19:41:51.320552Z","shell.execute_reply":"2020-12-03T19:41:51.319742Z"},"papermill":{"duration":3.449721,"end_time":"2020-12-03T19:41:51.320687","exception":false,"start_time":"2020-12-03T19:41:47.870966","status":"completed"},"tags":[],"id":"KquhUKBI5Qu9","executionInfo":{"status":"ok","timestamp":1608744584504,"user_tz":480,"elapsed":9224,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}}},"source":["import sys\n","sys.path.insert(0, \"timm-efficientdet-pytorch\")\n","sys.path.insert(0, \"omegaconf\")\n","\n","import torch\n","import os\n","import re\n","from datetime import datetime\n","import time\n","import random\n","import cv2\n","import pandas as pd\n","import numpy as np\n","import albumentations as A\n","import matplotlib.pyplot as plt\n","from albumentations.pytorch.transforms import ToTensorV2\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.model_selection import GroupKFold\n","from torch.utils.data import Dataset,DataLoader\n","from torch.utils.data.sampler import SequentialSampler, RandomSampler\n","from glob import glob\n","import pandas as pd\n","from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain, DetBenchEval\n","from effdet.efficientdet import HeadNet\n","from tqdm import tqdm\n","\n","SEED = 42\n","\n","def seed_everything(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","seed_everything(SEED)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.015808,"end_time":"2020-12-03T19:41:51.352887","exception":false,"start_time":"2020-12-03T19:41:51.337079","status":"completed"},"tags":[],"id":"DVwFB2QR5Qu9"},"source":["# Data Preparation"]},{"cell_type":"code","metadata":{"id":"fUPe5jAeDhXM","executionInfo":{"status":"ok","timestamp":1608744584505,"user_tz":480,"elapsed":9217,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}}},"source":["if Kaggle:\n","    BASEPATH = \"../input/nfl-impact-detection\"\n","    outdir = '.'\n","    TRAIN_IMGPATH = outdir\n","    MODELS_PATH = '../input/ed6-512-k5-aug3'\n","else:\n","    PATH = 'NFL/'\n","    BASEPATH = PATH + 'Data'\n","    TRAIN_IMGPATH = BASEPATH\n","    outdir = Path(PATH+'res')\n","    if not os.path.exists(outdir):\n","        os.mkdir(outdir)\n","    outdir = Path(PATH+'res/efficientDet-res')\n","    if not os.path.exists(outdir):\n","        os.mkdir(outdir)\n","    MODELNAME = \"ED5-512\"\n","    # MODELNAME = \"ED4-512\"\n","    VERSION = '{}'.format(MODELNAME)\n","    outdir = os.path.join(outdir, VERSION)\n","    if not os.path.exists(outdir):\n","        os.mkdir(outdir)\n","    # from datetime import datetime, timedelta\n","    # dateTimeObj = datetime.now()\n","    # timestampStr = dateTimeObj.strftime(\"%d-%b-%Y-%H\")\n","    # modelpath = os.path.join(outdir, 'all-kfold-hairaug-456-norm-metanew-b16')\n","    timestampStr = 'org-epoch20-aug3'\n","    outdir = os.path.join(outdir, timestampStr)\n","    if not os.path.exists(outdir):\n","        os.mkdir(outdir)  \n","    MODELS_PATH = outdir\n","SCORE_TH = 0.4\n","SKIP_SCORE = 0.4        "],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"LEuE8MRJdW62","executionInfo":{"status":"ok","timestamp":1608744584505,"user_tz":480,"elapsed":9210,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}}},"source":["def mk_images(video_name, video_labels, video_dir, out_dir, only_with_impact=True):\n","    video_path=f\"{video_dir}/{video_name}\"\n","    video_name = os.path.basename(video_path)\n","    vidcap = cv2.VideoCapture(video_path)\n","    if only_with_impact:\n","        boxes_all = video_labels.query(\"video == @video_name\")\n","        print(video_path, boxes_all[boxes_all.impact == 1.0].shape[0])\n","    else:\n","        print(video_path)\n","    frame = 0\n","    while True:\n","        it_worked, img = vidcap.read()\n","        if not it_worked:\n","            break\n","        frame += 1\n","        if only_with_impact:\n","            boxes = video_labels.query(\"video == @video_name and frame == @frame\")\n","            boxes_with_impact = boxes[boxes.impact == 1.0]\n","            if boxes_with_impact.shape[0] == 0:\n","                continue\n","        img_name = f\"{video_name}_frame{frame}\"\n","        image_path = f'{out_dir}/{video_name}'.replace('.mp4',f'_{frame}.png')\n","        _ = cv2.imwrite(image_path, img)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"SbPskHEMdg7Z","executionInfo":{"status":"ok","timestamp":1608744584506,"user_tz":480,"elapsed":9204,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}}},"source":["# out_dir = DATA_ROOT_PATH\n","# if not os.path.exists(out_dir):\n","#     !mkdir -p $out_dir\n","#     video_dir = '/kaggle/input/nfl-impact-detection/test'\n","#     uniq_video = [path.split('/')[-1] for path in glob(f'{video_dir}/*.mp4')]\n","#     for video_name in uniq_video:\n","#         mk_images(video_name, pd.DataFrame(), video_dir, out_dir, only_with_impact=False)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-12-03T19:41:51.397809Z","iopub.status.busy":"2020-12-03T19:41:51.397205Z","iopub.status.idle":"2020-12-03T19:54:33.739582Z","shell.execute_reply":"2020-12-03T19:54:33.739088Z"},"papermill":{"duration":762.371229,"end_time":"2020-12-03T19:54:33.739714","exception":false,"start_time":"2020-12-03T19:41:51.368485","status":"completed"},"tags":[],"id":"0QPqdU4t5Qu9","colab":{"base_uri":"https://localhost:8080/","height":241},"executionInfo":{"status":"ok","timestamp":1608744585668,"user_tz":480,"elapsed":10356,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}},"outputId":"cdb51dd2-5d01-4c4b-ee44-336bb1b4b991"},"source":["# video_labels = pd.read_csv(os.path.join(BASEPATH, 'train_labels.csv')).fillna(0)\n","# video_labels_with_impact = video_labels[video_labels['impact'] > 0]\n","# for row in tqdm(video_labels_with_impact[['video','frame','label']].values):\n","#     frames = np.array([-4,-3,-2,-1,1,2,3,4])+row[1]\n","#     video_labels.loc[(video_labels['video'] == row[0]) \n","#                                  & (video_labels['frame'].isin(frames))\n","#                                  & (video_labels['label'] == row[2]), 'impact'] = 1\n","# video_labels['image_name'] = video_labels['video'].str.replace('.mp4', '') + '_' + video_labels['frame'].astype(str) + '.png'\n","# video_labels = video_labels[video_labels.groupby('image_name')['impact'].transform(\"sum\") > 0].reset_index(drop=True)\n","# video_labels['impact'] = video_labels['impact'].astype(int)+1\n","# video_labels['x'] = video_labels['left']\n","# video_labels['y'] = video_labels['top']\n","# video_labels['w'] = video_labels['width']\n","# video_labels['h'] = video_labels['height']\n","# video_labels.head()\n","\n","video_labels = pd.read_csv(os.path.join(BASEPATH,'video_labels.csv'), index_col=0)\n","video_labels.loc[:,'VID'] = ''\n","video_labels.loc[:, 'VID'] =  ['_'.join(re.split('_|\\\\.', vid)[0:2]) for vid in video_labels['video']]\n","display(video_labels.head())\n","print(video_labels.shape)"],"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>gameKey</th>\n","      <th>playID</th>\n","      <th>view</th>\n","      <th>video</th>\n","      <th>frame</th>\n","      <th>label</th>\n","      <th>left</th>\n","      <th>width</th>\n","      <th>top</th>\n","      <th>height</th>\n","      <th>impact</th>\n","      <th>impactType</th>\n","      <th>confidence</th>\n","      <th>visibility</th>\n","      <th>image_name</th>\n","      <th>x</th>\n","      <th>y</th>\n","      <th>w</th>\n","      <th>h</th>\n","      <th>VID</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>57583</td>\n","      <td>82</td>\n","      <td>Endzone</td>\n","      <td>57583_000082_Endzone.mp4</td>\n","      <td>34</td>\n","      <td>V73</td>\n","      <td>655</td>\n","      <td>21</td>\n","      <td>331</td>\n","      <td>15</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>57583_000082_Endzone_34.png</td>\n","      <td>655</td>\n","      <td>331</td>\n","      <td>21</td>\n","      <td>15</td>\n","      <td>57583_000082</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>57583</td>\n","      <td>82</td>\n","      <td>Endzone</td>\n","      <td>57583_000082_Endzone.mp4</td>\n","      <td>34</td>\n","      <td>H99</td>\n","      <td>583</td>\n","      <td>21</td>\n","      <td>312</td>\n","      <td>30</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>57583_000082_Endzone_34.png</td>\n","      <td>583</td>\n","      <td>312</td>\n","      <td>21</td>\n","      <td>30</td>\n","      <td>57583_000082</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>57583</td>\n","      <td>82</td>\n","      <td>Endzone</td>\n","      <td>57583_000082_Endzone.mp4</td>\n","      <td>34</td>\n","      <td>V15</td>\n","      <td>1069</td>\n","      <td>22</td>\n","      <td>301</td>\n","      <td>20</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>57583_000082_Endzone_34.png</td>\n","      <td>1069</td>\n","      <td>301</td>\n","      <td>22</td>\n","      <td>20</td>\n","      <td>57583_000082</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>57583</td>\n","      <td>82</td>\n","      <td>Endzone</td>\n","      <td>57583_000082_Endzone.mp4</td>\n","      <td>34</td>\n","      <td>H97</td>\n","      <td>402</td>\n","      <td>21</td>\n","      <td>313</td>\n","      <td>29</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>57583_000082_Endzone_34.png</td>\n","      <td>402</td>\n","      <td>313</td>\n","      <td>21</td>\n","      <td>29</td>\n","      <td>57583_000082</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>57583</td>\n","      <td>82</td>\n","      <td>Endzone</td>\n","      <td>57583_000082_Endzone.mp4</td>\n","      <td>34</td>\n","      <td>V72</td>\n","      <td>445</td>\n","      <td>21</td>\n","      <td>328</td>\n","      <td>16</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>57583_000082_Endzone_34.png</td>\n","      <td>445</td>\n","      <td>328</td>\n","      <td>21</td>\n","      <td>16</td>\n","      <td>57583_000082</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   gameKey  playID     view  ...   w   h           VID\n","0    57583      82  Endzone  ...  21  15  57583_000082\n","1    57583      82  Endzone  ...  21  30  57583_000082\n","2    57583      82  Endzone  ...  22  20  57583_000082\n","3    57583      82  Endzone  ...  21  29  57583_000082\n","4    57583      82  Endzone  ...  21  16  57583_000082\n","\n","[5 rows x 20 columns]"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["(197838, 20)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5RRgNTn4CLmB","executionInfo":{"status":"ok","timestamp":1608744585668,"user_tz":480,"elapsed":10344,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}}},"source":[""],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"LB91hjvzSIcI","executionInfo":{"status":"ok","timestamp":1608744585669,"user_tz":480,"elapsed":10338,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}}},"source":["# display(video_labels.head())\n","# print(video_labels.shape)\n","# video_labels.to_csv(os.path.join(BASEPATH,'video_labels.csv'), index=False)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-12-03T19:54:35.361621Z","iopub.status.busy":"2020-12-03T19:54:35.360784Z","iopub.status.idle":"2020-12-03T19:54:35.468122Z","shell.execute_reply":"2020-12-03T19:54:35.467547Z"},"papermill":{"duration":0.925308,"end_time":"2020-12-03T19:54:35.468243","exception":false,"start_time":"2020-12-03T19:54:34.542935","status":"completed"},"tags":[],"id":"d8dFJux45Qu9","executionInfo":{"status":"ok","timestamp":1608744585669,"user_tz":480,"elapsed":10331,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}}},"source":["# np.random.seed(0)\n","# video_names = np.random.permutation(video_labels.video.unique())\n","# valid_video_len = int(len(video_names)*0.2)\n","# video_valid = video_names[:valid_video_len]\n","# video_train = video_names[valid_video_len:]\n","# images_valid = video_labels[ video_labels.video.isin(video_valid)].image_name.unique()\n","# images_train = video_labels[~video_labels.video.isin(video_valid)].image_name.unique()"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d1Kjkk_KuY8G"},"source":["\n","# Stratified K-Fold\n"]},{"cell_type":"code","metadata":{"id":"9nHFZBzzuXf3","executionInfo":{"status":"ok","timestamp":1608744588068,"user_tz":480,"elapsed":12723,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}}},"source":["# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n","gkf = GroupKFold(n_splits=5)\n","df_folds = video_labels[['image_name']].copy()\n","df_folds.loc[:, 'bbox_count'] = 1\n","df_folds = df_folds.groupby('image_name').count()\n","df_folds.loc[:, 'video'] = video_labels[['image_name', 'video']].groupby('image_name').min()['video']\n","# print(display(df_folds))\n","\n","# df_folds.loc[:, 'stratify_group'] = np.char.add(\n","#     df_folds['video'].values.astype(str),\n","#     df_folds['bbox_count'].apply(lambda x: f'_{x // 20}').values.astype(str),\n","# )\n","df_folds.loc[:, 'stratify_group'] = video_labels[['image_name', 'VID']].groupby('image_name').min()['VID']\n","df_folds.loc[:, 'fold'] = 0\n","# print(display(df_folds))\n","# for fold_number, (train_index, val_index) in enumerate(skf.split(X=df_folds.index, y=df_folds['stratify_group'])):\n","#     df_folds.loc[df_folds.iloc[val_index].index, 'fold'] = fold_number\n","for fold_number, (train_index, val_index) in enumerate(gkf.split(X=df_folds.index, groups=df_folds['stratify_group'])):\n","    df_folds.loc[df_folds.iloc[val_index].index, 'fold'] = fold_number\n","\n","# print(display(df_folds))\n"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"GSZqDmyM72iO","executionInfo":{"status":"ok","timestamp":1608744588069,"user_tz":480,"elapsed":12716,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}}},"source":["# print(video_labels[['VID', 'video']])"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"taF2BRlSLGfQ","executionInfo":{"status":"ok","timestamp":1608744588070,"user_tz":480,"elapsed":12707,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}},"outputId":"8b3b6ac5-79ae-407a-955a-01e201707280"},"source":["for i in range(5):\n","    print(list(df_folds['fold'].values).count(i))"],"execution_count":13,"outputs":[{"output_type":"stream","text":["2006\n","2004\n","2006\n","2003\n","1998\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"n8Zo1hGPR0t_","executionInfo":{"status":"ok","timestamp":1608744588583,"user_tz":480,"elapsed":13206,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}},"outputId":"c1fcb28e-ae66-4f33-fe2b-3adbc0942fb7"},"source":["# df_folds.stratify_group.unique()\n","video_fold_dic = pd.DataFrame(columns=['VID','fold'])\n","for vid in df_folds.stratify_group.unique():\n","    fold = df_folds.fold[df_folds.stratify_group==vid].unique()\n","    # print(vid, fold)\n","    dftemp = pd.DataFrame({'VID': [vid],\n","                    'fold': [fold[0]]})\n","    video_fold_dic = video_fold_dic.append(dftemp, ignore_index= True)\n","\n","display(video_fold_dic.head())"],"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>VID</th>\n","      <th>fold</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>57583_000082</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>57584_000336</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>57584_002674</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>57586_000540</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>57586_001934</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            VID fold\n","0  57583_000082    1\n","1  57584_000336    2\n","2  57584_002674    2\n","3  57586_000540    4\n","4  57586_001934    2"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"Jz7DD4N_PO8m","executionInfo":{"status":"ok","timestamp":1608744588584,"user_tz":480,"elapsed":13196,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}}},"source":["# vid_list = video_fold_dic['VID'][video_fold_dic['fold']==0]\n","# print(vid_list)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2020-12-03T19:54:38.758603Z","iopub.status.busy":"2020-12-03T19:54:38.757834Z","iopub.status.idle":"2020-12-03T20:17:19.848931Z","shell.execute_reply":"2020-12-03T20:17:19.848179Z"},"papermill":{"duration":1361.975668,"end_time":"2020-12-03T20:17:19.849108","exception":false,"start_time":"2020-12-03T19:54:37.873440","status":"completed"},"tags":[],"id":"mF1JI58F5Qu-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608744588584,"user_tz":480,"elapsed":13185,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}},"outputId":"77543d66-2015-4bad-956a-49c8dde7f23f"},"source":["uniq_video = video_labels.video.unique()\n","# video_dir = '/kaggle/input/nfl-impact-detection/train'\n","video_dir = os.path.join(BASEPATH, 'train')\n","out_dir_total = os.path.join(TRAIN_IMGPATH, 'train_images_total')\n","if not os.path.exists(out_dir_total):\n","    !mkdir -p $out_dir_total\n","    uniq_video = [path.split('/')[-1] for path in glob(f'{video_dir}/*.mp4')]\n","    for video_name in uniq_video:\n","        mk_images(video_name, pd.DataFrame(), video_dir, out_dir_total, only_with_impact=False)\n","\n","print(out_dir_total)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["NFL/Data/train_images_total\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"papermill":{"duration":1.116956,"end_time":"2020-12-03T20:17:21.805986","exception":false,"start_time":"2020-12-03T20:17:20.689030","status":"completed"},"tags":[],"id":"uZDPyNTA5Qu-"},"source":["## Albumentations"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-12-03T20:17:23.531452Z","iopub.status.busy":"2020-12-03T20:17:23.529545Z","iopub.status.idle":"2020-12-03T20:17:23.535342Z","shell.execute_reply":"2020-12-03T20:17:23.534636Z"},"papermill":{"duration":0.894654,"end_time":"2020-12-03T20:17:23.535470","exception":false,"start_time":"2020-12-03T20:17:22.640816","status":"completed"},"tags":[],"id":"RBeM0iN85Qu-","executionInfo":{"status":"ok","timestamp":1608744588585,"user_tz":480,"elapsed":13176,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}}},"source":["def get_train_transforms():\n","    return A.Compose(\n","        [\n","            A.HorizontalFlip(p=0.5),\n","            # A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=10, interpolation=1, border_mode=4, \n","            #                    value=None, mask_value=None, always_apply=False, p=0.5),\n","            A.OneOf([\n","              A.RandomSizedCrop(min_max_height=(500, 720), height=720, width=720, p=1.0),\n","              A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=10, interpolation=1, border_mode=4, \n","                                value=None, mask_value=None, always_apply=False, p=1.0),\n","            ], p=0.5),\n","            A.OneOf([\n","                A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, \n","                                     val_shift_limit=0.2, p=0.9),\n","                A.RandomBrightnessContrast(brightness_limit=0.2, \n","                                           contrast_limit=0.2, p=0.9),\n","            ], p=0.4),\n","            A.JpegCompression(quality_lower=85, quality_upper=95, p=0.2),\n","            A.Resize(height=512, width=512, p=1),\n","            A.OneOf([\n","                A.Blur(blur_limit=3, p=1.0),\n","                A.MedianBlur(blur_limit=3, p=1.0),\n","            ],p=0.1),\n","            A.Cutout(num_holes=8, max_h_size=64, max_w_size=64, fill_value=0, p=0.3),\n","            # A.Blur(blur_limit=5, always_apply=False, p=0.1),\n","            # A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, always_apply=False, p=0.1),\n","            ToTensorV2(p=1.0),\n","        ], \n","        p=1.0, \n","        bbox_params=A.BboxParams(\n","            format='pascal_voc',\n","            min_area=0, \n","            min_visibility=0,\n","            label_fields=['labels']\n","        )\n","    )\n","\n","def get_valid_transforms():\n","    return A.Compose(\n","        [\n","            A.Resize(height=512, width=512, p=1.0),\n","            ToTensorV2(p=1.0),\n","        ], \n","        p=1.0, \n","        bbox_params=A.BboxParams(\n","            format='pascal_voc',\n","            min_area=0, \n","            min_visibility=0,\n","            label_fields=['labels']\n","        )\n","    )\n","\n","def get_ooftest_valid_transforms():\n","    return A.Compose([\n","            A.Resize(height=512, width=512, p=1.0),\n","            ToTensorV2(p=1.0),\n","        ], p=1.0)"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.860904,"end_time":"2020-12-03T20:17:25.233882","exception":false,"start_time":"2020-12-03T20:17:24.372978","status":"completed"},"tags":[],"id":"gYqmaqmX5Qu-"},"source":["## Dataset"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-12-03T20:17:27.167718Z","iopub.status.busy":"2020-12-03T20:17:27.166491Z","iopub.status.idle":"2020-12-03T20:17:27.179341Z","shell.execute_reply":"2020-12-03T20:17:27.178834Z"},"papermill":{"duration":1.00411,"end_time":"2020-12-03T20:17:27.179451","exception":false,"start_time":"2020-12-03T20:17:26.175341","status":"completed"},"tags":[],"id":"PpEmFOv55Qu-","executionInfo":{"status":"ok","timestamp":1608744588829,"user_tz":480,"elapsed":13402,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}}},"source":["TRAIN_ROOT_PATH = os.path.join(TRAIN_IMGPATH, 'train_images')\n","TRAIN_VAL_ROOT_PATH = os.path.join(TRAIN_IMGPATH, 'train_images_total')\n","class DatasetRetriever(Dataset):\n","\n","    def __init__(self, marking, image_ids, transforms=None, test=False):\n","        super().__init__()\n","\n","        self.image_ids = image_ids\n","        self.marking = marking\n","        self.transforms = transforms\n","        self.test = test\n","\n","    def __getitem__(self, index: int):\n","        image_id = self.image_ids[index]\n","        \n","        # image, boxes, labels = self.load_image_and_boxes(index)\n","        if self.test or random.random() > 0.5:\n","            image, boxes, labels = self.load_image_and_boxes(index)\n","        elif random.random() > 0.33:\n","            image, boxes, labels = self.load_cutmix_image_and_boxes(index)\n","        else:\n","            image, boxes, labels = self.load_mixup_image_and_boxes(index)\n","        \n","        target = {}\n","        target['boxes'] = boxes\n","        target['labels'] = torch.tensor(labels)\n","        target['image_id'] = torch.tensor([index])\n","\n","        if self.transforms:\n","            for i in range(10):\n","                sample = self.transforms(**{\n","                    'image': image,\n","                    'bboxes': target['boxes'],\n","                    'labels': labels\n","                })\n","                if len(sample['bboxes']) > 0:\n","                    image = sample['image']\n","                    target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n","                    target['boxes'][:,[0,1,2,3]] = target['boxes'][:,[1,0,3,2]]  #yxyx: be warning\n","                    break\n","        return image, target, image_id\n","\n","    def __len__(self) -> int:\n","        return self.image_ids.shape[0]\n","\n","    def load_image_and_boxes(self, index):\n","        image_id = self.image_ids[index]\n","        # print(f'{TRAIN_ROOT_PATH}/{image_id}')\n","        image = cv2.imread(f'{TRAIN_ROOT_PATH}/{image_id}', cv2.IMREAD_COLOR).copy().astype(np.float32)\n","        # img = cv2.imread(f'{TRAIN_ROOT_PATH}/{image_id}', cv2.IMREAD_COLOR)\n","        # if img is NoneType:\n","        #     print(f'{TRAIN_ROOT_PATH}/{image_id}')\n","        # image = img.copy().astype(np.float32)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n","        image /= 255.0\n","        records = self.marking[self.marking['image_name'] == image_id]\n","        boxes = records[['x', 'y', 'w', 'h']].values\n","        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n","        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n","        labels = records['impact'].values\n","        return image, boxes, labels\n","\n","    def load_mixup_image_and_boxes(self, index):\n","        image, boxes, labels = self.load_image_and_boxes(index)\n","        r_image, r_boxes, r_labels = self.load_image_and_boxes(random.randint(0, self.image_ids.shape[0] - 1))\n","        return (image+r_image)/2, np.vstack((boxes, r_boxes)).astype(np.int32), np.concatenate((labels, r_labels))\n","    \n","\n","    def load_cutmix_image_and_boxes(self, index, imsize=720):\n","        \"\"\" \n","        This implementation of cutmix author:  https://www.kaggle.com/nvnnghia \n","        Refactoring and adaptation: https://www.kaggle.com/shonenkov\n","        \"\"\"\n","        w, h = imsize, imsize\n","        s = imsize // 2\n","    \n","        xc, yc = [int(random.uniform(imsize * 0.25, imsize * 0.75)) for _ in range(2)]  # center x, y\n","        indexes = [index] + [random.randint(0, self.image_ids.shape[0] - 1) for _ in range(3)]\n","\n","        result_image = np.full((imsize, imsize, 3), 1, dtype=np.float32)\n","        result_boxes = []\n","        result_labels = np.array([], dtype=np.int)\n","\n","        for i, index in enumerate(indexes):\n","            image, boxes, labels = self.load_image_and_boxes(index)\n","            if i == 0:\n","                x1a, y1a, x2a, y2a = max(xc - w, 0), max(yc - h, 0), xc, yc  # xmin, ymin, xmax, ymax (large image)\n","                x1b, y1b, x2b, y2b = w - (x2a - x1a), h - (y2a - y1a), w, h  # xmin, ymin, xmax, ymax (small image)\n","            elif i == 1:  # top right\n","                x1a, y1a, x2a, y2a = xc, max(yc - h, 0), min(xc + w, s * 2), yc\n","                x1b, y1b, x2b, y2b = 0, h - (y2a - y1a), min(w, x2a - x1a), h\n","            elif i == 2:  # bottom left\n","                x1a, y1a, x2a, y2a = max(xc - w, 0), yc, xc, min(s * 2, yc + h)\n","                x1b, y1b, x2b, y2b = w - (x2a - x1a), 0, max(xc, w), min(y2a - y1a, h)\n","            elif i == 3:  # bottom right\n","                x1a, y1a, x2a, y2a = xc, yc, min(xc + w, s * 2), min(s * 2, yc + h)\n","                x1b, y1b, x2b, y2b = 0, 0, min(w, x2a - x1a), min(y2a - y1a, h)\n","            result_image[y1a:y2a, x1a:x2a] = image[y1b:y2b, x1b:x2b]\n","            padw = x1a - x1b\n","            padh = y1a - y1b\n","\n","            boxes[:, 0] += padw\n","            boxes[:, 1] += padh\n","            boxes[:, 2] += padw\n","            boxes[:, 3] += padh\n","\n","            result_boxes.append(boxes)\n","            result_labels = np.concatenate((result_labels, labels))\n","\n","        result_boxes = np.concatenate(result_boxes, 0)\n","        np.clip(result_boxes[:, 0:], 0, 2 * s, out=result_boxes[:, 0:])\n","        result_boxes = result_boxes.astype(np.int32)\n","        index_to_use = np.where((result_boxes[:,2]-result_boxes[:,0])*(result_boxes[:,3]-result_boxes[:,1]) > 0)\n","        result_boxes = result_boxes[index_to_use]\n","        result_labels = result_labels[index_to_use]\n","        \n","        return result_image, result_boxes, result_labels"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZTqoNcPnkcUk","executionInfo":{"status":"ok","timestamp":1608744588830,"user_tz":480,"elapsed":13394,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}}},"source":["class TestDatasetRetriever(Dataset):\n","    def __init__(self, image_ids, transforms=None):\n","        super().__init__()\n","        self.image_ids = image_ids\n","        self.transforms = transforms\n","\n","    def __getitem__(self, index: int):\n","        image_id = self.image_ids[index]\n","        image = cv2.imread(f'{TRAIN_VAL_ROOT_PATH}/{image_id}', cv2.IMREAD_COLOR).copy().astype(np.float32)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n","        image /= 255.0\n","        if self.transforms:\n","            sample = {'image': image}\n","            sample = self.transforms(**sample)\n","            image = sample['image']\n","        return image, image_id\n","\n","    def __len__(self) -> int:\n","        return self.image_ids.shape[0]"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"ga6331RBkjt3","executionInfo":{"status":"ok","timestamp":1608744588830,"user_tz":480,"elapsed":13386,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}}},"source":[""],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-12-03T20:17:29.786515Z","iopub.status.busy":"2020-12-03T20:17:29.785545Z","iopub.status.idle":"2020-12-03T20:17:29.787381Z","shell.execute_reply":"2020-12-03T20:17:29.788067Z"},"papermill":{"duration":1.299025,"end_time":"2020-12-03T20:17:29.788236","exception":false,"start_time":"2020-12-03T20:17:28.489211","status":"completed"},"tags":[],"id":"zfBRK8395Qu-","executionInfo":{"status":"ok","timestamp":1608744588831,"user_tz":480,"elapsed":13379,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}}},"source":["# train_dataset = DatasetRetriever(\n","#     # image_ids=images_train,\n","#     image_ids=df_folds[df_folds['fold'] != fold_number].index.values,\n","#     marking=video_labels,\n","#     transforms=get_train_transforms(),\n","#     test=False,\n","# )\n","\n","# validation_dataset = DatasetRetriever(\n","#     # image_ids=images_valid,\n","#     image_ids=df_folds[df_folds['fold'] == fold_number].index.values,\n","#     marking=video_labels,\n","#     transforms=get_valid_transforms(),\n","#     test=True,\n","# )"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.854518,"end_time":"2020-12-03T20:17:31.516845","exception":false,"start_time":"2020-12-03T20:17:30.662327","status":"completed"},"tags":[],"id":"fR6RQZ_B5Qu-"},"source":["## Fitter"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-12-03T20:17:33.240203Z","iopub.status.busy":"2020-12-03T20:17:33.238373Z","iopub.status.idle":"2020-12-03T20:17:33.240858Z","shell.execute_reply":"2020-12-03T20:17:33.241329Z"},"papermill":{"duration":0.857125,"end_time":"2020-12-03T20:17:33.241450","exception":false,"start_time":"2020-12-03T20:17:32.384325","status":"completed"},"tags":[],"id":"2Ds6QgCh5Qu-","executionInfo":{"status":"ok","timestamp":1608744588831,"user_tz":480,"elapsed":13371,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}}},"source":["class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-12-03T20:17:34.928263Z","iopub.status.busy":"2020-12-03T20:17:34.893824Z","iopub.status.idle":"2020-12-03T20:17:34.943197Z","shell.execute_reply":"2020-12-03T20:17:34.942605Z"},"papermill":{"duration":0.879671,"end_time":"2020-12-03T20:17:34.943328","exception":false,"start_time":"2020-12-03T20:17:34.063657","status":"completed"},"tags":[],"id":"pY_EQeAl5Qu-","executionInfo":{"status":"ok","timestamp":1608744588832,"user_tz":480,"elapsed":13363,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}}},"source":["import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","class Fitter:\n","    \n","    def __init__(self, model, device, config, checkpointfile, train_on_checkpoint):\n","        self.config = config\n","        self.epoch = 0\n","\n","        self.base_dir = f'./{config.folder}'\n","        if not os.path.exists(self.base_dir):\n","            os.makedirs(self.base_dir)\n","        \n","        self.log_path = f'{self.base_dir}/log.txt'\n","        self.best_summary_loss = 10**5\n","\n","        self.model = model\n","        self.device = device\n","\n","        param_optimizer = list(self.model.named_parameters())\n","        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","        optimizer_grouped_parameters = [\n","            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n","            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","        ] \n","\n","        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=config.lr)\n","        self.scheduler = config.SchedulerClass(self.optimizer, **config.scheduler_params)\n","        self.log(f'Fitter prepared. Device is {self.device}')\n","        self.checkpointfile = checkpointfile\n","        self.train_on_checkpoint = train_on_checkpoint\n","\n","    def fit(self, train_loader, validation_loader):\n","        if self.train_on_checkpoint:\n","            self.load(self.checkpointfile)\n","            print(\"Loading from pretrained-model: {}\".format(self.checkpointfile))\n","        for e in range(self.config.n_epochs):\n","            if self.config.verbose:\n","                lr = self.optimizer.param_groups[0]['lr']\n","                timestamp = datetime.utcnow().isoformat()\n","                self.log(f'\\n{timestamp}\\nLR: {lr}')\n","\n","            t = time.time()\n","            summary_loss = self.train_one_epoch(train_loader)\n","\n","            self.log(f'[RESULT]: Train. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, time: {(time.time() - t):.5f}')\n","            self.save(f'{self.base_dir}/last-checkpoint.bin')\n","\n","            t = time.time()\n","            summary_loss = self.validation(validation_loader)\n","\n","            self.log(f'[RESULT]: Val. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, time: {(time.time() - t):.5f}')\n","            if summary_loss.avg < self.best_summary_loss:\n","                self.best_summary_loss = summary_loss.avg\n","                self.model.eval()\n","                self.save(f'{self.base_dir}/best-checkpoint-{str(self.epoch).zfill(3)}epoch.bin')\n","                for path in sorted(glob(f'{self.base_dir}/best-checkpoint-*epoch.bin'))[:-3]:\n","                    os.remove(path)\n","\n","            if self.config.validation_scheduler:\n","                self.scheduler.step(metrics=summary_loss.avg)\n","\n","            self.epoch += 1\n","\n","    def validation(self, val_loader):\n","        self.model.eval()\n","        summary_loss = AverageMeter()\n","        t = time.time()\n","        for step, (images, targets, image_ids) in enumerate(val_loader):\n","            if self.config.verbose:\n","                if step % self.config.verbose_step == 0:\n","                    print(\n","                        f'Val Step {step}/{len(val_loader)}, ' + \\\n","                        f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n","                        f'time: {(time.time() - t):.5f}', end='\\r'\n","                    )\n","            with torch.no_grad():\n","                images = torch.stack(images)\n","                batch_size = images.shape[0]\n","                images = images.to(self.device).float()\n","                boxes = [target['boxes'].to(self.device).float() for target in targets]\n","                labels = [target['labels'].to(self.device).float() for target in targets]\n","\n","                loss, _, _ = self.model(images, boxes, labels)\n","                summary_loss.update(loss.detach().item(), batch_size)\n","\n","        return summary_loss\n","\n","    def train_one_epoch(self, train_loader):\n","        self.model.train()\n","        summary_loss = AverageMeter()\n","        t = time.time()\n","        for step, (images, targets, image_ids) in enumerate(train_loader):\n","            if self.config.verbose:\n","                if step % self.config.verbose_step == 0:\n","                    print(\n","                        f'Train Step {step}/{len(train_loader)}, ' + \\\n","                        f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n","                        f'time: {(time.time() - t):.5f}', end='\\r'\n","                    )\n","            \n","            images = torch.stack(images)\n","            images = images.to(self.device).float()\n","            batch_size = images.shape[0]\n","            boxes = [target['boxes'].to(self.device).float() for target in targets]\n","            labels = [target['labels'].to(self.device).float() for target in targets]\n","\n","            self.optimizer.zero_grad()\n","            \n","            loss, _, _ = self.model(images, boxes, labels)\n","            \n","            loss.backward()\n","\n","            summary_loss.update(loss.detach().item(), batch_size)\n","\n","            self.optimizer.step()\n","\n","            if self.config.step_scheduler:\n","                self.scheduler.step()\n","\n","        return summary_loss\n","    \n","    def save(self, path):\n","        self.model.eval()\n","        torch.save({\n","            'model_state_dict': self.model.model.state_dict(),\n","            'optimizer_state_dict': self.optimizer.state_dict(),\n","            'scheduler_state_dict': self.scheduler.state_dict(),\n","            'best_summary_loss': self.best_summary_loss,\n","            'epoch': self.epoch,\n","        }, path)\n","\n","    def load(self, path):\n","        checkpoint = torch.load(path)\n","        self.model.model.load_state_dict(checkpoint['model_state_dict'])\n","        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n","        self.best_summary_loss = checkpoint['best_summary_loss']\n","        self.epoch = checkpoint['epoch'] + 1\n","        \n","    def log(self, message):\n","        if self.config.verbose:\n","            print(message)\n","        with open(self.log_path, 'a+') as logger:\n","            logger.write(f'{message}\\n')"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-12-03T20:17:36.640201Z","iopub.status.busy":"2020-12-03T20:17:36.639432Z","iopub.status.idle":"2020-12-03T20:17:36.643624Z","shell.execute_reply":"2020-12-03T20:17:36.643063Z"},"papermill":{"duration":0.840689,"end_time":"2020-12-03T20:17:36.643746","exception":false,"start_time":"2020-12-03T20:17:35.803057","status":"completed"},"tags":[],"id":"Qu5pIEq85Qu-","executionInfo":{"status":"ok","timestamp":1608744588832,"user_tz":480,"elapsed":13353,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}}},"source":["class TrainGlobalConfig:\n","    num_workers = 4\n","    batch_size = 4 \n","    val_batch_size = 16\n","    n_epochs = 5\n","    lr = 0.0002\n","    folder = os.path.join(outdir, 'effdet5-models')\n","    verbose = True\n","    verbose_step = 1\n","    step_scheduler = False\n","    validation_scheduler = True\n","    SchedulerClass = torch.optim.lr_scheduler.ReduceLROnPlateau\n","    scheduler_params = dict(\n","        mode='min',\n","        factor=0.5,\n","        patience=1,\n","        verbose=False, \n","        threshold=0.0001,\n","        threshold_mode='abs',\n","        cooldown=0, \n","        min_lr=1e-8,\n","        eps=1e-08\n","    )"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"NB_9UvPwPU9e","executionInfo":{"status":"ok","timestamp":1608744588833,"user_tz":480,"elapsed":13346,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}}},"source":[""],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-12-03T20:17:38.345119Z","iopub.status.busy":"2020-12-03T20:17:38.344232Z","iopub.status.idle":"2020-12-03T20:17:38.347364Z","shell.execute_reply":"2020-12-03T20:17:38.346780Z"},"papermill":{"duration":0.856628,"end_time":"2020-12-03T20:17:38.347461","exception":false,"start_time":"2020-12-03T20:17:37.490833","status":"completed"},"tags":[],"id":"3pDRCaNk5Qu-","executionInfo":{"status":"ok","timestamp":1608744588833,"user_tz":480,"elapsed":13338,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}}},"source":["def collate_fn(batch):\n","    return tuple(zip(*batch))\n","\n","# def run_training():\n","#     if Kaggle:\n","#         device = torch.device('cuda:0')\n","#     else:\n","#         device = ('cuda' if torch.cuda.is_available() else 'cpu')\n","#     net.to(device)\n","\n","#     train_loader = torch.utils.data.DataLoader(\n","#         train_dataset,\n","#         batch_size=TrainGlobalConfig.batch_size,\n","#         sampler=RandomSampler(train_dataset),\n","#         pin_memory=False,\n","#         drop_last=True,\n","#         num_workers=TrainGlobalConfig.num_workers,\n","#         collate_fn=collate_fn,\n","#     )\n","#     val_loader = torch.utils.data.DataLoader(\n","#         validation_dataset, \n","#         batch_size=TrainGlobalConfig.batch_size,\n","#         num_workers=TrainGlobalConfig.num_workers,\n","#         shuffle=False,\n","#         sampler=SequentialSampler(validation_dataset),\n","#         pin_memory=False,\n","#         collate_fn=collate_fn,\n","#     )\n","\n","#     fitter = Fitter(model=net, device=device, config=TrainGlobalConfig)\n","#     fitter.fit(train_loader, val_loader)"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"GtyjfptzzAbA","executionInfo":{"status":"ok","timestamp":1608744588833,"user_tz":480,"elapsed":13330,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}}},"source":["def make_predictions_withnet(net_, images, score_threshold=0.5):\n","    images = torch.stack(images).cuda().float()\n","    box_list = []\n","    score_list = []\n","    class_list = []\n","    with torch.no_grad():\n","        det = net_(images, torch.tensor([1]*images.shape[0]).float().cuda())\n","        for i in range(images.shape[0]):\n","            boxes = det[i].detach().cpu().numpy()[:,:4]    \n","            scores = det[i].detach().cpu().numpy()[:,4]   \n","            label = det[i].detach().cpu().numpy()[:,5]\n","            # useing only label = 2\n","            indexes = np.where((scores > score_threshold) & (label == 2))[0]\n","            boxes[:, 2] = boxes[:, 2] + boxes[:, 0]\n","            boxes[:, 3] = boxes[:, 3] + boxes[:, 1]\n","            if len(boxes[indexes]) == 0:\n","                continue\n","            box_list.append(boxes[indexes])\n","            score_list.append(scores[indexes])\n","            class_list.append(label[indexes]) \n","    return box_list, class_list, score_list\n","\n","def make_predictions_withnet_imglist(net_, images, image_ids, score_threshold=0.5):\n","    images = torch.stack(images).cuda().float()\n","    box_list = []\n","    score_list = []\n","    class_list = []\n","    img_id_list = []\n","    with torch.no_grad():\n","        det = net_(images, torch.tensor([1]*images.shape[0]).float().cuda())\n","        for i in range(images.shape[0]):\n","            boxes = det[i].detach().cpu().numpy()[:,:4]    \n","            scores = det[i].detach().cpu().numpy()[:,4]   \n","            label = det[i].detach().cpu().numpy()[:,5]\n","            # useing only label = 2\n","            indexes = np.where((scores > score_threshold) & (label == 2))[0]\n","            boxes[:, 2] = boxes[:, 2] + boxes[:, 0]\n","            boxes[:, 3] = boxes[:, 3] + boxes[:, 1]\n","            \n","            if len(boxes[indexes]) == 0:\n","                continue\n","            box_list.append(boxes[indexes])\n","            score_list.append(scores[indexes])\n","            class_list.append(label[indexes]) \n","            img_id_list.append(image_ids[i])\n","            # print(img_id_list[-1])\n","    return box_list, class_list, score_list, img_id_list    \n","\n","def make_predictions_withnet_loder(net_, data_loader_, score_threshold=0.5):   \n","    box_list = []\n","    score_list = []\n","    class_list = []\n","    image_id_list = []\n","    for images, image_ids in data_loader_:\n","        box_list_, class_list_, score_list_ = make_predictions_withnet(net_, images, score_threshold=score_threshold)\n","        box_list = box_list + box_list_\n","        score_list = score_list + score_list_\n","        class_list = class_list + class_list_\n","        image_id_list = image_id_list + [id for id in image_ids]\n","    return box_list, class_list, score_list, image_id_list\n","\n","def make_predictions_validationset_withnet_loader(net_, data_loader_, val_gt_loader_, score_threshold=0.5):   \n","    box_list = []\n","    score_list = []\n","    class_list = []\n","    image_id_list = []\n","    val_img_id_list = []\n","    val_targets_list = []\n","    for images, image_ids in data_loader_:\n","        box_list_, class_list_, score_list_, img_id_list_ = make_predictions_withnet_imglist(net_, images, image_ids, score_threshold=score_threshold)\n","        box_list = box_list + box_list_\n","        score_list = score_list + score_list_\n","        class_list = class_list + class_list_\n","        # target_boxes = [target['boxes'].float() for target in targets]\n","        # targets_list = targets_list + target_boxes\n","        image_ids_index = [int(re.split('_|\\\\.',id)[3]) for id in img_id_list_]\n","        image_id_list = image_id_list + [id for id in image_ids_index]\n","    \n","    for images, targets, image_ids in val_gt_loader_:\n","        boxes = [target['boxes'].float() for target in targets]\n","        labels = [target['labels'].float() for target in targets]\n","        image_ids_index = [int(re.split('_|\\\\.',id)[3]) for id in image_ids]\n","        val_img_id_list = val_img_id_list + [id for id in image_ids_index]\n","        val_targets_list = val_targets_list + boxes\n","\n","    return box_list, class_list, score_list, image_id_list, val_img_id_list, val_targets_list"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"ovFTphqN4Iht","executionInfo":{"status":"ok","timestamp":1608744588835,"user_tz":480,"elapsed":13324,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}}},"source":["def transfer_for_f1(pred_list, gt_list, pred_frame_id_list, gt_frame_id_list):\n","    pred_frame_num = len(pred_frame_id_list)\n","    print(\"Pred Frame num: \", pred_frame_num)\n","    gt_frame_num = len(gt_frame_id_list)\n","    print(\"Gt Frame num: \", gt_frame_num)\n","    if pred_frame_num != len(pred_list) or gt_frame_num != len(gt_list):\n","        print(\"ERROR in transfer_for_f1!!!!!!!!\")\n","        return pred_list, gt_list, pred_frame_id_list, gt_frame_id_list\n","\n","    preds_total = []\n","    gts_total = []\n","    preds = []\n","    gts = []\n","    new_video = True\n","    k = 0\n","    for i in range(len(pred_frame_id_list)):\n","        frame_id = pred_frame_id_list[i]\n","        new_video = (frame_id == 1)\n","        if new_video:\n","            if len(preds) > 0:\n","                print(\"pred {} video processed done, len preds {}:\".format(k, len(preds)))\n","                preds_total.append(preds)\n","                k = k + 1\n","            preds = []\n","        preds.append([[frame_id, pred[0],pred[1],pred[2],pred[3]] for pred in pred_list[i] if len(pred_list[i])])\n","    if len(preds) > 0:\n","        preds_total.append(preds)\n","        print(\"pred {} video processed done, len preds {}:\".format(k, len(preds)))\n","        k = k + 1\n","\n","    new_video = True\n","    k = 0\n","    for i in range(len(gt_frame_id_list)):\n","        frame_id = gt_frame_id_list[i]\n","        new_video = (i==0) or (i>0 and i < len(gt_frame_id_list)-2 and gt_frame_id_list[i] < gt_frame_id_list[i-1] and gt_frame_id_list[i] < gt_frame_id_list[i+1])\n","        if new_video:\n","            if len(gts) > 0:\n","                print(\"gt {} video processed done, len preds {}:\".format(k, len(gts)))\n","                gts_total.append(gts)\n","                k = k + 1\n","            gts = []\n","        gts.append([[frame_id, gt[0],gt[1],gt[2],gt[3]] for gt in gt_list[i]])\n","    if len(gts) > 0:\n","        gts_total.append(gts)\n","        print(\"gt {} video processed done, len preds {}:\".format(k, len(gts)))\n","        k = k + 1\n","\n","    print(\"preds_total len:\", len(preds_total))\n","    print(\"gts_total len:\", len(gts_total))\n","    return preds_total, gts_total"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"sGDuZx-3O_Um","executionInfo":{"status":"ok","timestamp":1608744588836,"user_tz":480,"elapsed":13315,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}}},"source":["# a = []\n","# asub = []\n","# # asub.append([0,[g,g,g,g]] for g in range(4))\n","# # asub.append([0,[g,g,g,g]] for g in range(2))\n","# asub.append([[0, (g+i for i in range(4))]  for g in range(4)])\n","# asub.append([[0,g+1,g+2,g+3,g+4] for g in range(2)])\n","# print(asub)\n","# a.append(asub)\n","# # a = [[[0,1,1,1,1],[0,2,2,2,2],[0,4,4,4,4]],[[0,1,1,1,1],[0,2,2,2,2]]]\n","# np.save('test.npy',a)\n"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6TYkUDIT_1HX","executionInfo":{"status":"ok","timestamp":1608744588836,"user_tz":480,"elapsed":13306,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}},"outputId":"54248a86-03d4-4645-a0bb-d287a27739a8"},"source":["a = [[3, id] for id in range(3)]\n","print(a)\n","a = []\n","if len(a)>0:\n","  print(\"21\")\n","else:\n","  print(\"12\")"],"execution_count":28,"outputs":[{"output_type":"stream","text":["[[3, 0], [3, 1], [3, 2]]\n","12\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZnwXYnQxXgQJ","executionInfo":{"status":"ok","timestamp":1608744588837,"user_tz":480,"elapsed":13296,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}}},"source":["def load_net(checkpoint_path):\n","    config = get_efficientdet_config('tf_efficientdet_d5')\n","    net = EfficientDet(config, pretrained_backbone=False)\n","    config.num_classes = 2\n","    config.image_size=512\n","    net.class_net = HeadNet(config, num_outputs=config.num_classes, norm_kwargs=dict(eps=.001, momentum=.01))\n","    checkpoint = torch.load(checkpoint_path)\n","    net.load_state_dict(checkpoint['model_state_dict'])\n","    net = DetBenchEval(net, config)\n","    net.eval();\n","    return net.cuda()"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"id":"ONVHmZBbX7Hu","executionInfo":{"status":"ok","timestamp":1608744589111,"user_tz":480,"elapsed":13562,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}}},"source":["def select_train_video_ids(fold_number):\n","    image_ids=np.array([path.split('/')[-1] for path in glob(f'{TRAIN_VAL_ROOT_PATH}/*.png')])\n","    print(\"image total number:\", len(image_ids))\n","    # video_labels.loc[:, 'VID'] =  ['_'.join(re.split('_|\\\\.', vid)[0:2]) for vid in video_labels['video']]\n","    image_ids_vid = ['_'.join(re.split('_|\\\\.', imgid)[0:2]) for imgid in image_ids]\n","    vid_list = video_fold_dic['VID'][video_fold_dic['fold']==fold_number].values\n","    # print(vid_list)\n","    print(\"video list length:\", len(vid_list))\n","    # print(np.unique(image_ids_vid))\n","    return np.array([image_ids[i] for i in range(len(image_ids)) if image_ids_vid[i] in vid_list])"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"id":"SqIytc00zi-2","executionInfo":{"status":"ok","timestamp":1608744589111,"user_tz":480,"elapsed":13554,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}}},"source":["def run_training_kfold(num_fold, train_or_predict = True):\n","    # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    #device = torch.device('cuda:0')\n","    TRAIN_ON_CHECKPOINT = True\n","    if Kaggle:\n","        device = torch.device('cuda:0')\n","    else:\n","        device = ('cuda' if torch.cuda.is_available() else 'cpu')\n","    boxes = []\n","    targets = []\n","    pred_ids = []\n","    gt_ids = []\n","    for fold_number in range(num_fold):\n","        print('Fold: {}'.format(fold_number + 1))\n","        if fold_number == 1:\n","            break\n","        # if fold_number==4:\n","        #     TRAIN_ON_CHECKPOINT = True\n","        # else:\n","        #     TRAIN_ON_CHECKPOINT = False\n","        print(\"TRAIN_ON_CHECKPOINT:\", TRAIN_ON_CHECKPOINT)\n","        train_dataset = DatasetRetriever(\n","            image_ids=df_folds[df_folds['fold'] != fold_number].index.values,\n","            marking=video_labels,\n","            transforms=get_train_transforms(),\n","            test=False,\n","        )\n","        # image_ids=df_folds[df_folds['fold'] == fold_number].index.values\n","        # print(\"Val dataset imgid:\", image_ids)\n","        # print(\"Val dataset imgid type:\", type(image_ids))\n","        validation_dataset = DatasetRetriever(\n","            image_ids=df_folds[df_folds['fold'] == fold_number].index.values,\n","            marking=video_labels,\n","            transforms=get_valid_transforms(),\n","            test=True,\n","        )\n","        # print(\"preparing dataset done!\")\n","        train_loader = torch.utils.data.DataLoader(\n","            train_dataset,\n","            batch_size=TrainGlobalConfig.batch_size,\n","            sampler=RandomSampler(train_dataset),\n","            pin_memory=False,\n","            drop_last=True,\n","            num_workers=TrainGlobalConfig.num_workers,\n","            collate_fn=collate_fn,\n","        )\n","        val_loader = torch.utils.data.DataLoader(\n","            validation_dataset, \n","            batch_size=TrainGlobalConfig.batch_size,\n","            num_workers=TrainGlobalConfig.num_workers,\n","            shuffle=False,\n","            sampler=SequentialSampler(validation_dataset),\n","            pin_memory=False,\n","            collate_fn=collate_fn,\n","        )\n","        print(\"selecting test image ids...\")\n","        test_image_ids = select_train_video_ids(fold_number)\n","        # print(test_image_ids)\n","        # print(type(test_image_ids))\n","        print(\"selecting {} images for fold {}\".format(len(test_image_ids),fold_number))\n","        test_dataset = TestDatasetRetriever(\n","            # image_ids=np.array([path.split('/')[-1] for path in glob(f'{TRAIN_ROOT_PATH}/*.png')]),\n","            # image_ids=df_folds[df_folds['fold'] == fold_number].index.values,\n","            image_ids = test_image_ids,\n","            transforms=get_ooftest_valid_transforms()\n","        )\n","        test_loader = torch.utils.data.DataLoader(\n","            test_dataset,\n","            batch_size=16,\n","            shuffle=False,\n","            num_workers=4,\n","            drop_last=False,\n","            collate_fn=collate_fn\n","        )\n","        # print(\"preparing loader done\")\n","        if train_or_predict:\n","            net, checkpoint_file = get_net(fold_number, TRAIN_ON_CHECKPOINT)\n","            net.to(device)\n","            TrainGlobalConfig.folder = os.path.join(outdir, f'effdet5-models/fold{fold_number}')\n","            fitter = Fitter(model=net, device=device, config=TrainGlobalConfig, checkpointfile=checkpoint_file, train_on_checkpoint = TRAIN_ON_CHECKPOINT)\n","            fitter.fit(train_loader, val_loader)\n","        else:\n","            print(\"WE ARE GOING TO CALCULATE CV SCORES FOR EACH VIDEOS!!!!\")\n","            if Colab:\n","                models_list = os.listdir(os.path.join(MODELS_PATH, f'effdet5-models/fold{fold_number}'))\n","                # print(\"all files in models path\", models_list)\n","                models_list = [i for i in models_list if \"best\" in i]\n","                fold_best_model = os.path.join(MODELS_PATH, f'effdet5-models/fold{fold_number}', models_list[-1])\n","            else:\n","                models_list = os.listdir(os.path.join(MODELS_PATH))\n","                models_list = [i for i in models_list if f'fold{fold_number}' in i]\n","                fold_best_model = os.path.join(MODELS_PATH, models_list[-1])\n","            print(\"loading model name: \", fold_best_model)\n","            net_ = load_net(fold_best_model)\n","            net_.to(device)  \n","            box_list, class_list, score_list, image_id_list, val_img_id_list, val_targets_list = make_predictions_validationset_withnet_loader(net_, test_loader, val_loader, score_threshold=SCORE_TH)\n","            boxes = boxes + box_list\n","            targets = targets + val_targets_list\n","            pred_ids = pred_ids + image_id_list\n","            gt_ids = gt_ids + val_img_id_list\n","\n","    if not train_or_predict:\n","        print(\"Transfer to cv score data....\")\n","        preds, gts = transfer_for_f1(boxes, targets, pred_ids, gt_ids)\n","        np.save(os.path.join(outdir, 'oof-gt.npy'), gts)\n","        np.save(os.path.join(outdir, 'oof-pred.npy'), preds)\n","        np.savez(os.path.join(outdir, 'oof'), gts=gts, preds=preds) \n","        print(\"Transmation done!!\")            "],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"id":"tzN5ZnYWVejJ","executionInfo":{"status":"ok","timestamp":1608744589112,"user_tz":480,"elapsed":13547,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}}},"source":[""],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"id":"5nIg9gfCSAkV","executionInfo":{"status":"ok","timestamp":1608744589112,"user_tz":480,"elapsed":13540,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}}},"source":["def iou(bbox1, bbox2):\n","    bbox1 = [float(x) for x in bbox1]\n","    bbox2 = [float(x) for x in bbox2]\n","\n","    (x0_1, y0_1, x1_1, y1_1) = bbox1\n","    (x0_2, y0_2, x1_2, y1_2) = bbox2\n","\n","    # get the overlap rectangle\n","    overlap_x0 = max(x0_1, x0_2)\n","    overlap_y0 = max(y0_1, y0_2)\n","    overlap_x1 = min(x1_1, x1_2)\n","    overlap_y1 = min(y1_1, y1_2)\n","\n","    # check if there is an overlap\n","    if overlap_x1 - overlap_x0 <= 0 or overlap_y1 - overlap_y0 <= 0:\n","            return 0\n","\n","    # if yes, calculate the ratio of the overlap to each ROI size and the unified size\n","    size_1 = (x1_1 - x0_1) * (y1_1 - y0_1)\n","    size_2 = (x1_2 - x0_2) * (y1_2 - y0_2)\n","    size_intersection = (overlap_x1 - overlap_x0) * (overlap_y1 - overlap_y0)\n","    size_union = size_1 + size_2 - size_intersection\n","\n","    return size_intersection / size_union"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y9HD9a2uSBNU","executionInfo":{"status":"ok","timestamp":1608744589113,"user_tz":480,"elapsed":13533,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}}},"source":["def precision_calc(gt_boxes, pred_boxes):\n","    cost_matix = np.ones((len(gt_boxes), len(pred_boxes)))\n","    for i, box1 in enumerate(gt_boxes):\n","        for j, box2 in enumerate(pred_boxes):\n","            dist = abs(box1[0]-box2[0])\n","            if dist > 4:\n","                continue\n","            iou_score = iou(box1[1:], box2[1:])\n","\n","            if iou_score < 0.35:\n","                continue\n","            else:\n","                cost_matix[i,j]=0\n","\n","    row_ind, col_ind = linear_sum_assignment(cost_matix)\n","    fn = len(gt_boxes) - row_ind.shape[0]\n","    fp = len(pred_boxes) - col_ind.shape[0]\n","    tp=0\n","    for i, j in zip(row_ind, col_ind):\n","        if cost_matix[i,j]==0:\n","            tp+=1\n","        else:\n","            fp+=1\n","            fn+=1\n","    return tp, fp, fn"],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"id":"34ZLkZjIRzGj","executionInfo":{"status":"ok","timestamp":1608744589113,"user_tz":480,"elapsed":13526,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}}},"source":["def cal_cvscore(oof_file = os.path.join(outdir, 'oof.npz'), oof_gt_file = os.path.join(outdir, 'oof-gt.npy'), oof_pred_file = os.path.join(outdir, 'oof-pred.npy')):\n","    #Calculating CV score.\n","    testdata = np.load(oof_file, allow_pickle=True)\n","    preds = np.load(oof_pred_file, allow_pickle=True)\n","    gts = np.load(oof_gt_file, allow_pickle=True)\n","    ftp, ffp, ffn = [], [], []\n","    # for count, data in enumerate(testdata):\n","    #     print(count)\n","    #     print(data)\n","    #     gt_boxes = data['gts']\n","    #     print(gt_boxes)\n","    #     pred_boxes = data['preds']\n","    #     tp, fp, fn = precision_calc(gt_boxes, pred_boxes)\n","    #     ftp.append(tp)\n","    #     ffp.append(fp)\n","    #     ffn.append(fn)\n","\n","    for i in range(len(preds)):\n","        gt_boxes = gts[i]\n","        print(\"gt_boxes in video{} is len {}\".format(i, len(gt_boxes)))\n","        print(gt_boxes[0][:3])\n","        pred_boxes = preds[i]\n","        print(pred_boxes[:15])\n","        print(\"pred_boxes in video{} is len {}\".format(i, len(pred_boxes)))\n","        tp, fp, fn = precision_calc(gt_boxes, pred_boxes)\n","        ftp.append(tp)\n","        ffp.append(fp)\n","        ffn.append(fn)\n","\n","    tp = np.sum(ftp)\n","    fp = np.sum(ffp)\n","    fn = np.sum(ffn)\n","    precision = tp / (tp + fp + 1e-6)\n","    recall =  tp / (tp + fn +1e-6)\n","    f1_score = 2*(precision*recall)/(precision+recall+1e-6)\n","    print(f'TP: {tp}, FP: {fp}, FN: {fn}, PRECISION: {precision:.4f}, RECALL: {recall:.4f}, F1 SCORE: {f1_score}')"],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-12-03T20:17:40.490346Z","iopub.status.busy":"2020-12-03T20:17:40.480631Z","iopub.status.idle":"2020-12-03T20:17:44.356599Z","shell.execute_reply":"2020-12-03T20:17:44.355423Z"},"papermill":{"duration":4.767308,"end_time":"2020-12-03T20:17:44.356749","exception":false,"start_time":"2020-12-03T20:17:39.589441","status":"completed"},"tags":[],"id":"nVTdclME5Qu-","executionInfo":{"status":"ok","timestamp":1608744589114,"user_tz":480,"elapsed":13521,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}}},"source":["def get_net(fold_number, train_on_checkpoint):\n","    # config = get_efficientdet_config('tf_efficientdet_d5')\n","    # print(\"Enter get_net, TRAIN_ON_CHECKPOINT:\", train_on_checkpoint)\n","    config = get_efficientdet_config('tf_efficientdet_d5')\n","\n","    net = EfficientDet(config, pretrained_backbone=False)\n","\n","    if train_on_checkpoint:\n","        # print(outdir)\n","        checkpointfile = os.path.join(outdir, f'effdet5-models/fold{fold_number}/last-checkpoint.bin')\n","    else:\n","        checkpoint = torch.load(os.path.join(PATH, 'nfl-models/efficientdet_d5-ef44aea8.pth'))\n","        net.load_state_dict(checkpoint)\n","        checkpointfile = ''\n","    print(\"checkpointfile:\", checkpointfile)\n","    # checkpoint = torch.load(os.path.join(PATH, 'nfl-models/efficientdet_d5-ef44aea8.pth'))\n","    # checkpoint = torch.load(os.path.join(PATH, 'nfl-models/efficientdet_d4-5b370b7a.pth'))\n","    # net.load_state_dict(checkpoint)\n","    config.num_classes = 2\n","    config.image_size = 512\n","    net.class_net = HeadNet(config, num_outputs=config.num_classes, norm_kwargs=dict(eps=.001, momentum=.01))\n","    return DetBenchTrain(net, config), checkpointfile\n","\n","# net, checkpointfile = get_net()"],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-12-03T20:17:46.495960Z","iopub.status.busy":"2020-12-03T20:17:46.069898Z","iopub.status.idle":"2020-12-03T21:15:33.319215Z","shell.execute_reply":"2020-12-03T21:15:33.318628Z"},"papermill":{"duration":3468.111783,"end_time":"2020-12-03T21:15:33.319336","exception":false,"start_time":"2020-12-03T20:17:45.207553","status":"completed"},"scrolled":true,"tags":[],"id":"lNN08JJW5Qu-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608746762982,"user_tz":480,"elapsed":2187380,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}},"outputId":"040ef616-d95a-4922-f22c-536264127a98"},"source":["# run_training()\n","run_training_kfold(num_fold=5, train_or_predict = TRAIN)"],"execution_count":36,"outputs":[{"output_type":"stream","text":["Fold: 1\n","TRAIN_ON_CHECKPOINT: True\n","selecting test image ids...\n","image total number: 44448\n","video list length: 12\n","selecting 8060 images for fold 0\n","WE ARE GOING TO CALCULATE CV SCORES FOR EACH VIDEOS!!!!\n","loading model name:  NFL/res/efficientDet-res/ED5-512/org-epoch20-aug3/effdet5-models/fold0/best-checkpoint-045epoch.bin\n","Fold: 2\n","Transfer to cv score data....\n","Pred Frame num:  2637\n","Gt Frame num:  2006\n","pred 0 video processed done, len preds 465:\n","pred 1 video processed done, len preds 1072:\n","pred 2 video processed done, len preds 540:\n","pred 3 video processed done, len preds 177:\n","pred 4 video processed done, len preds 383:\n","gt 0 video processed done, len preds 76:\n","gt 1 video processed done, len preds 19:\n","gt 2 video processed done, len preds 4:\n","gt 3 video processed done, len preds 62:\n","gt 4 video processed done, len preds 19:\n","gt 5 video processed done, len preds 4:\n","gt 6 video processed done, len preds 54:\n","gt 7 video processed done, len preds 92:\n","gt 8 video processed done, len preds 74:\n","gt 9 video processed done, len preds 83:\n","gt 10 video processed done, len preds 94:\n","gt 11 video processed done, len preds 92:\n","gt 12 video processed done, len preds 2:\n","gt 13 video processed done, len preds 95:\n","gt 14 video processed done, len preds 79:\n","gt 15 video processed done, len preds 54:\n","gt 16 video processed done, len preds 59:\n","gt 17 video processed done, len preds 64:\n","gt 18 video processed done, len preds 56:\n","gt 19 video processed done, len preds 45:\n","gt 20 video processed done, len preds 13:\n","gt 21 video processed done, len preds 6:\n","gt 22 video processed done, len preds 47:\n","gt 23 video processed done, len preds 16:\n","gt 24 video processed done, len preds 4:\n","gt 25 video processed done, len preds 96:\n","gt 26 video processed done, len preds 101:\n","gt 27 video processed done, len preds 81:\n","gt 28 video processed done, len preds 78:\n","gt 29 video processed done, len preds 87:\n","gt 30 video processed done, len preds 64:\n","gt 31 video processed done, len preds 71:\n","gt 32 video processed done, len preds 5:\n","gt 33 video processed done, len preds 8:\n","gt 34 video processed done, len preds 9:\n","gt 35 video processed done, len preds 5:\n","gt 36 video processed done, len preds 8:\n","gt 37 video processed done, len preds 4:\n","gt 38 video processed done, len preds 10:\n","gt 39 video processed done, len preds 7:\n","gt 40 video processed done, len preds 9:\n","gt 41 video processed done, len preds 4:\n","gt 42 video processed done, len preds 54:\n","gt 43 video processed done, len preds 5:\n","gt 44 video processed done, len preds 9:\n","gt 45 video processed done, len preds 8:\n","gt 46 video processed done, len preds 6:\n","gt 47 video processed done, len preds 7:\n","gt 48 video processed done, len preds 3:\n","gt 49 video processed done, len preds 7:\n","gt 50 video processed done, len preds 9:\n","gt 51 video processed done, len preds 6:\n","gt 52 video processed done, len preds 7:\n","gt 53 video processed done, len preds 25:\n","preds_total len: 5\n","gts_total len: 54\n","Transmation done!!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"c_whrUM37IyY","executionInfo":{"status":"ok","timestamp":1608746762983,"user_tz":480,"elapsed":2187371,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}}},"source":["# print(gts)"],"execution_count":37,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Q1xvuo_XAZk","colab":{"base_uri":"https://localhost:8080/","height":446},"executionInfo":{"status":"error","timestamp":1608746771759,"user_tz":480,"elapsed":2196140,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}},"outputId":"b20caece-d5fd-4f35-c5a4-05d1545c54c1"},"source":["if not TRAIN:\n","    print(\"Calculating cv scores!!\")\n","    cal_cvscore()"],"execution_count":38,"outputs":[{"output_type":"stream","text":["Calculating cv scores!!\n","gt_boxes in video0 is len 76\n","[[103, tensor(131.5556), tensor(258.8000), tensor(147.2000), tensor(267.6000)], [103, tensor(188.4444), tensor(142.4000), tensor(209.0667), tensor(153.2000)], [103, tensor(210.4889), tensor(119.6000), tensor(226.1333), tensor(128.8000)]]\n","[[[440, 300.46188, 274.1194, 308.27188, 286.67383]], [[448, 289.10715, 275.25616, 297.90555, 293.18146]], [[449, 291.15408, 276.40753, 299.4592, 293.5833], [449, 299.3325, 285.54956, 305.62302, 293.66235]], [[450, 289.16653, 276.67752, 297.9338, 294.15665], [450, 300.111, 285.03403, 306.54904, 294.02066]], [[451, 289.34167, 276.16528, 298.05408, 293.84802], [451, 300.25223, 284.88904, 306.9608, 294.2168]], [[452, 299.32935, 284.06546, 306.1618, 294.11008], [452, 289.89648, 277.66656, 298.23322, 294.22247]], [[41, 222.14024, 211.70863, 230.51152, 229.62093]], [[42, 224.18207, 216.90019, 230.89047, 229.73042]], [[43, 223.27008, 218.63528, 229.75018, 230.53311]], [[44, 220.6662, 219.92197, 227.13757, 231.54681]], [[45, 221.17764, 220.08278, 227.14105, 230.791]], [[46, 221.95126, 221.05093, 227.13092, 229.9353]], [[47, 223.1605, 221.21068, 228.52892, 229.72322], [47, 269.43872, 235.57703, 276.0443, 245.15039]], [[48, 223.53319, 220.60803, 229.17671, 229.21115], [48, 271.61472, 217.98775, 279.8771, 236.56276], [48, 268.39807, 232.23483, 275.40082, 242.57361], [48, 228.10878, 210.50557, 236.14217, 227.43138]], [[49, 271.75763, 220.62523, 279.2409, 236.12715], [49, 223.91324, 219.9699, 229.89459, 228.84502], [49, 329.86652, 223.0729, 336.58118, 233.4255], [49, 269.1228, 230.61253, 275.61877, 240.36101], [49, 325.15045, 214.25188, 333.164, 233.0522]]]\n","pred_boxes in video0 is len 465\n"],"name":"stdout"},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-38-4d1b2955ffc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTRAIN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Calculating cv scores!!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mcal_cvscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-34-6b3c9f172344>\u001b[0m in \u001b[0;36mcal_cvscore\u001b[0;34m(oof_file, oof_gt_file, oof_pred_file)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_boxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pred_boxes in video{} is len {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_boxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_calc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_boxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mftp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mffp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-33-7800014411f9>\u001b[0m in \u001b[0;36mprecision_calc\u001b[0;34m(gt_boxes, pred_boxes)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_boxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_boxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m             \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbox2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'list' and 'list'"]}]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-12-03T21:15:39.110558Z","iopub.status.busy":"2020-12-03T21:15:39.109713Z","iopub.status.idle":"2020-12-03T21:15:40.239972Z","shell.execute_reply":"2020-12-03T21:15:40.239159Z"},"papermill":{"duration":3.984352,"end_time":"2020-12-03T21:15:40.240095","exception":false,"start_time":"2020-12-03T21:15:36.255743","status":"completed"},"tags":[],"id":"TduA6fs65Qu-","executionInfo":{"status":"aborted","timestamp":1608746771757,"user_tz":480,"elapsed":2196130,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}}},"source":["# clearing working dir\n","# be careful when running this code on local environment!\n","# !rm -rf *\n","# !mv * /tmp/train_images\n","\n","# import shutil\n","# shutil.rmtree(out_dir)  "],"execution_count":null,"outputs":[]}]}